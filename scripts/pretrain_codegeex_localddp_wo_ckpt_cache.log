[2024-03-31 16:37:51,788] [WARNING] [runner.py:155:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-03-31 16:37:51,861] [INFO] [runner.py:453:main] cmd = /data0/anaconda3/envs/csw_codegeex/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29501 /data0/csw/CodeGeeX/codegeex/megatron/tools/pretrain_codegeex.py --tensor-model-parallel-size 4 --pipeline-model-parallel-size 1 --no-pipeline-parallel --num-layers 39 --hidden-size 5120 --make-vocab-size-divisible-by 52224 --num-attention-heads 40 --seq-length 512 --loss-scale 12 --max-position-embeddings 2048 --micro-batch-size 2 --global-batch-size 4 --train-iters 25 --lr 2e-4 --min-lr 1e-7 --lr-decay-iters 100000 --lr-decay-style cosine --lr-warmup-iters 1500 --log-interval 1 --eval-iters 10 --eval-interval 10 --data-path /data0/csw/CodeGeeX/pt_data/my_data --data-impl mmap --vocab-file /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json --merge-file /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt --save-interval 100 --save /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test --load-state /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/ --split 100,0,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --fp16 --ln-fp16 --attention-softmax-in-fp32 --checkpoint-activations --override-lr-scheduler --tensorboard-dir /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240331_163750 --use-contiguous-buffers-in-ddp --accumulate-allreduce-grads-in-fp32
[2024-03-31 16:37:53,075] [INFO] [launch.py:96:main] 0 NCCL_IB_GID_INDEX=3
[2024-03-31 16:37:53,075] [INFO] [launch.py:96:main] 0 NCCL_IB_DISABLE=0
[2024-03-31 16:37:53,075] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-03-31 16:37:53,076] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-03-31 16:37:53,076] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-03-31 16:37:53,076] [INFO] [launch.py:123:main] dist_world_size=8
[2024-03-31 16:37:53,076] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8, data-parallel-size: 2, tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... True
  beam_search ..................................... False
  beam_search_nucleus ............................. False
  beam_warmup ..................................... False
  beam_warmup_length .............................. 0
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  co_evaluation ................................... False
  compress ........................................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 2
  data_path ....................................... ['/data0/csw/CodeGeeX/pt_data/my_data']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... False
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ None
  deepspeed_mpi ................................... False
  dist_timeout .................................... 30
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 512
  eod_mask_loss ................................... False
  eval_interval ................................... 10
  eval_iters ...................................... 10
  evaluation ...................................... False
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 20480
  finetune ........................................ False
  force_default ................................... False
  force_device .................................... None
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 4
  gold ............................................ False
  gold_beta ....................................... 0.05
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 5120
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  index_cache_dir ................................. None
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  ln_fp16 ......................................... True
  load ............................................ None
  load_state ...................................... /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 12.0
  loss_scale_window ............................... 1000
  low_memory_load ................................. None
  lr .............................................. 0.0002
  lr_decay_iters .................................. 100000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 1500
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 52224
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-07
  mmap_warmup ..................................... False
  ms_model ........................................ False
  no_learned_position_embeddings .................. False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 40
  num_beams ....................................... 4
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 39
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  play_tau ........................................ 2.0
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  reward_growth ................................... constant
  sample_rate ..................................... 1.0
  save ............................................ /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
  save_interval ................................... 100
  scale_embeddings ................................ False
  scaled_upper_triang_masked_softmax_fusion ....... False
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 512
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  shrink_embedding_gradient_alpha ................. 1.0
  shrink_embedding_gradient_steps ................. None
  shrink_logit_embedding_gradient ................. False
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tempering ....................................... None
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240331_163750
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_path .................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  train_iters ..................................... 25
  train_samples ................................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... True
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  valid_data_path ................................. None
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json
  wandb_log_interval .............................. 1
  wandb_logging ................................... False
  weight_decay .................................... 0.1
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
 > padded vocab (size: 50257) with 1967 dummy tokens (new size: 52224)
  > (rank=7) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
> initializing torch distributed ...
  > (rank=4) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=0) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=5) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=1) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=3) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=6) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=2) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=0) process group initialized
> initializing tensor model parallel with size 4
> initializing pipeline model parallel with size 1
  > (rank=4) process group initialized
  > (rank=5) process group initialized
  > (rank=1) process group initialized  > (rank=3) process group initialized

  > (rank=6) process group initialized  > (rank=2) process group initialized

  > (rank=7) process group initialized
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
time to initialize megatron (seconds): -23.680
[after megatron is initialized] datetime: 2024-03-31 16:38:00 
Creating output dir ...
building GPT model ...
[2024-03-31 16:38:00,394] [INFO] [utils.py:828:see_memory_usage] Before Building Model
[2024-03-31 16:38:00,399] [INFO] [utils.py:829:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-31 16:38:00,399] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 61.8 GB, percent = 3.3%
Loading warmstarting model states ...
Loading model from /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/mp_rank_00_model_states.pt ...
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 3227279360
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
 > creating DDP model done
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 3227279360
 > moving model to GPU ...
 > converting to fp16 done
 > creating DDP model ...
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
 > creating DDP model done
 > creating DDP model done
[2024-03-31 16:38:06,753] [INFO] [utils.py:828:see_memory_usage] After Building Model
[2024-03-31 16:38:06,753] [INFO] [utils.py:829:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.23 GB         Max_CA 6 GB 
[2024-03-31 16:38:06,754] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 94.42 GB, percent = 5.1%
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
 > creating DDP model done
time (ms) | load-model-states: 5517.54
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3227279360
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3227279360
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
[2024-03-31 16:38:07,099] [INFO] [utils.py:828:see_memory_usage] Before moving to GPU
[2024-03-31 16:38:07,099] [INFO] [utils.py:829:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.23 GB         Max_CA 6 GB 
[2024-03-31 16:38:07,099] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 80.93 GB, percent = 4.3%
 > moving model to GPU ...
 > converting to fp16 done
 > creating DDP model ...
 > moving to GPU done
 > creating DDP model done
[2024-03-31 16:38:07,170] [INFO] [utils.py:828:see_memory_usage] After moving to GPU
[2024-03-31 16:38:07,171] [INFO] [utils.py:829:see_memory_usage] MA 6.01 GB         Max_MA 6.01 GB         CA 6.25 GB         Max_CA 6 GB 
[2024-03-31 16:38:07,171] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 78.76 GB, percent = 4.2%
 > moving model to GPU ...
[2024-03-31 16:38:07,215] [INFO] [utils.py:828:see_memory_usage] Before Float16Module Warpped
[2024-03-31 16:38:07,216] [INFO] [utils.py:829:see_memory_usage] MA 6.01 GB         Max_MA 6.01 GB         CA 6.25 GB         Max_CA 6 GB 
[2024-03-31 16:38:07,216] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.46 GB, percent = 4.1%
 > converting model to fp16 ...
 > moving model to GPU ...
 > converting to fp16 done
 > moving to GPU done
 > converting model to fp16 ...
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
 > converting to fp16 done
 > creating DDP model ...
 > creating DDP model done
 > creating DDP model done
[2024-03-31 16:38:07,268] [INFO] [utils.py:828:see_memory_usage] After Float16Module Warpped
[2024-03-31 16:38:07,269] [INFO] [utils.py:829:see_memory_usage] MA 6.01 GB         Max_MA 6.01 GB         CA 6.25 GB         Max_CA 6 GB 
[2024-03-31 16:38:07,269] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.39 GB, percent = 4.1%
 > creating DDP model ...
[2024-03-31 16:38:07,308] [INFO] [utils.py:828:see_memory_usage] Before LocalDDP Wrapped
[2024-03-31 16:38:07,309] [INFO] [utils.py:829:see_memory_usage] MA 6.01 GB         Max_MA 6.01 GB         CA 6.25 GB         Max_CA 6 GB 
[2024-03-31 16:38:07,309] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.39 GB, percent = 4.1%
 > creating DDP model done
[2024-03-31 16:38:07,372] [INFO] [utils.py:828:see_memory_usage] After LocalDDP Wrapped
[2024-03-31 16:38:07,373] [INFO] [utils.py:829:see_memory_usage] MA 18.04 GB         Max_MA 18.04 GB         CA 18.27 GB         Max_CA 18 GB 
[2024-03-31 16:38:07,373] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.4 GB, percent = 4.1%
[2024-03-31 16:38:07,414] [INFO] [utils.py:828:see_memory_usage] Before Adam Init
[2024-03-31 16:38:07,415] [INFO] [utils.py:829:see_memory_usage] MA 18.04 GB         Max_MA 18.04 GB         CA 18.27 GB         Max_CA 18 GB 
[2024-03-31 16:38:07,415] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.4 GB, percent = 4.1%
[2024-03-31 16:38:07,455] [INFO] [utils.py:828:see_memory_usage] After Adam Init
[2024-03-31 16:38:07,455] [INFO] [utils.py:829:see_memory_usage] MA 18.04 GB         Max_MA 18.04 GB         CA 18.27 GB         Max_CA 18 GB 
[2024-03-31 16:38:07,456] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.4 GB, percent = 4.1%
[2024-03-31 16:38:07,495] [INFO] [utils.py:828:see_memory_usage] Before Float16OptimizerWithFloat16Params Wrapped
[2024-03-31 16:38:07,496] [INFO] [utils.py:829:see_memory_usage] MA 18.04 GB         Max_MA 18.04 GB         CA 18.27 GB         Max_CA 18 GB 
[2024-03-31 16:38:07,496] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.4 GB, percent = 4.1%
[2024-03-31 16:38:07,648] [INFO] [utils.py:828:see_memory_usage] After Float16OptimizerWithFloat16Params Wrapped
[2024-03-31 16:38:07,649] [INFO] [utils.py:829:see_memory_usage] MA 30.14 GB         Max_MA 30.18 GB         CA 30.45 GB         Max_CA 30 GB 
[2024-03-31 16:38:07,649] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.4 GB, percent = 4.1%
> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-31 16:38:07 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      100
    validation: 120
    test:       40
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000195 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000088 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.016160 seconds
    number of documents: 500
 > dataset split:
    train:
     document indices in [0, 500) total of 500 documents
    validation:
     document indices in [500, 500) total of 0 documents
    test:
     document indices in [500, 500) total of 0 documents
 > loading doc-idx mapping from /data0/csw/CodeGeeX/pt_data/my_data_train_indexmap_100ns_512sl_1234s_doc_idx.npy
    total number of samples: 100
    total number of epochs: 1
train_dataset:<class 'codegeex.megatron.data.prompt_dataset.PromptDataset'>
valid_dataset:<class 'NoneType'>
test_dataset:<class 'NoneType'>
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-03-31 16:38:08 time (ms) | model-and-optimizer-setup: 7042.95 | train/valid/test-data-iterators-setup: 818.87

done with setup ...
training ...
[before the start of training step] datetime: 2024-03-31 16:38:08 
==> iteration        1/      25 | consumed samples:            4 | consumed tokens:         2048 | elapsed time per iteration (ms): 4632.6 | learning rate: 1.333E-07 | global batch size:     4 | lm loss: 3.924718E+00 | loss scale: 12.0 | grad norm: 732.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2813.70 | backward-compute: 165.40 | backward-params-all-reduce: 101.07 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.74 | optimizer-unscale-and-check-inf: 633.18 | optimizer-clip-main-grad: 29.19 | optimizer-copy-main-to-model-params: 19.58 | optimizer: 1543.51 | batch-generator: 6.50
[Rank 2] (after 1 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 55809.58251953125 | reserved: 56292.0 | max reserved: 56292.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 55809.58251953125 | reserved: 56306.0 | max reserved: 56306.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 55810.08251953125 | reserved: 56166.0 | max reserved: 56166.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 55809.58251953125 | reserved: 56306.0 | max reserved: 56306.0
==> iteration        2/      25 | consumed samples:            8 | consumed tokens:         4096 | elapsed time per iteration (ms): 500.0 | learning rate: 2.667E-07 | global batch size:     4 | lm loss: 3.960377E+00 | loss scale: 12.0 | grad norm: 642.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 2 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0[Rank 0] (after 2 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0

[Rank 3] (after 2 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 2 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
time (ms) | forward-compute: 93.39 | backward-compute: 165.00 | backward-params-all-reduce: 101.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.88 | optimizer-unscale-and-check-inf: 20.09 | optimizer-clip-main-grad: 28.54 | optimizer-copy-main-to-model-params: 19.56 | optimizer: 131.42 | batch-generator: 21.83
==> iteration        3/      25 | consumed samples:           12 | consumed tokens:         6144 | elapsed time per iteration (ms): 461.0 | learning rate: 4.000E-07 | global batch size:     4 | lm loss: 4.450614E+00 | loss scale: 12.0 | grad norm: 593.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.22 | backward-compute: 159.51 | backward-params-all-reduce: 100.90 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.87 | optimizer-unscale-and-check-inf: 18.87 | optimizer-clip-main-grad: 28.37 | optimizer-copy-main-to-model-params: 19.52 | optimizer: 129.88 | batch-generator: 0.73
[Rank 3] (after 3 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 3 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 1] (after 3 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 3 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
==> iteration        4/      25 | consumed samples:           16 | consumed tokens:         8192 | elapsed time per iteration (ms): 461.3 | learning rate: 5.333E-07 | global batch size:     4 | lm loss: 3.256634E+00 | loss scale: 12.0 | grad norm: 601.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 3] (after 4 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 4 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0[Rank 1] (after 4 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

[Rank 0] (after 4 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 62.53 | backward-compute: 158.84 | backward-params-all-reduce: 101.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.93 | optimizer-unscale-and-check-inf: 18.97 | optimizer-clip-main-grad: 28.45 | optimizer-copy-main-to-model-params: 19.52 | optimizer: 130.22 | batch-generator: 1.04
[Rank 2] (after 5 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
==> iteration        5/      25 | consumed samples:           20 | consumed tokens:        10240 | elapsed time per iteration (ms): 460.2 | learning rate: 6.667E-07 | global batch size:     4 | lm loss: 3.165217E+00 | loss scale: 12.0 | grad norm: 448.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 5 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 3] (after 5 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 5 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 61.77 | backward-compute: 158.88 | backward-params-all-reduce: 101.06 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.83 | optimizer-unscale-and-check-inf: 18.93 | optimizer-clip-main-grad: 28.43 | optimizer-copy-main-to-model-params: 19.62 | optimizer: 130.10 | batch-generator: 1.08
==> iteration        6/      25 | consumed samples:           24 | consumed tokens:        12288 | elapsed time per iteration (ms): 459.0 | learning rate: 8.000E-07 | global batch size:     4 | lm loss: 2.749013E+00 | loss scale: 12.0 | grad norm: 349.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 6 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 3] (after 6 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
time (ms) | forward-compute: 61.22 | backward-compute: 157.75 | backward-params-all-reduce: 101.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.87 | optimizer-unscale-and-check-inf: 19.05 | optimizer-clip-main-grad: 28.42 | optimizer-copy-main-to-model-params: 19.52 | optimizer: 130.23 | batch-generator: 0.89
[Rank 2] (after 6 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0[Rank 1] (after 6 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

==> iteration        7/      25 | consumed samples:           28 | consumed tokens:        14336 | elapsed time per iteration (ms): 461.0 | learning rate: 9.333E-07 | global batch size:     4 | lm loss: 2.593580E+00 | loss scale: 12.0 | grad norm: 290.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.92 | backward-compute: 158.96 | backward-params-all-reduce: 101.07 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.82 | optimizer-unscale-and-check-inf: 18.83 | optimizer-clip-main-grad: 28.28 | optimizer-copy-main-to-model-params: 19.44 | optimizer: 129.61 | batch-generator: 0.80
[Rank 3] (after 7 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 7 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 1] (after 7 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0[Rank 2] (after 7 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0

==> iteration        8/      25 | consumed samples:           32 | consumed tokens:        16384 | elapsed time per iteration (ms): 457.8 | learning rate: 1.067E-06 | global batch size:     4 | lm loss: 1.495719E+00 | loss scale: 12.0 | grad norm: 157.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.94 | backward-compute: 157.76 | backward-params-all-reduce: 100.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.80 | optimizer-unscale-and-check-inf: 18.89 | optimizer-clip-main-grad: 28.26 | optimizer-copy-main-to-model-params: 19.75 | optimizer: 130.01 | batch-generator: 0.80
[Rank 3] (after 8 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 8 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 1] (after 8 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 8 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
==> iteration        9/      25 | consumed samples:           36 | consumed tokens:        18432 | elapsed time per iteration (ms): 464.6 | learning rate: 1.200E-06 | global batch size:     4 | lm loss: 1.228085E+00 | loss scale: 12.0 | grad norm: 177.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 2] (after 9 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
[Rank 0] (after 9 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 3] (after 9 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 1] (after 9 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
time (ms) | forward-compute: 63.43 | backward-compute: 161.71 | backward-params-all-reduce: 100.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.89 | optimizer-unscale-and-check-inf: 19.05 | optimizer-clip-main-grad: 28.40 | optimizer-copy-main-to-model-params: 19.49 | optimizer: 130.07 | batch-generator: 1.33
==> iteration       10/      25 | consumed samples:           40 | consumed tokens:        20480 | elapsed time per iteration (ms): 458.8 | learning rate: 1.333E-06 | global batch size:     4 | lm loss: 5.464401E-01 | loss scale: 12.0 | grad norm: 66.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 10 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0[Rank 2] (after 10 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0

[Rank 3] (after 10 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 10 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 60.72 | backward-compute: 158.58 | backward-params-all-reduce: 101.22 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.82 | optimizer-unscale-and-check-inf: 18.95 | optimizer-clip-main-grad: 28.31 | optimizer-copy-main-to-model-params: 19.53 | optimizer: 129.83 | batch-generator: 0.91
[Rank 1] (after 11 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 11 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
==> iteration       11/      25 | consumed samples:           44 | consumed tokens:        22528 | elapsed time per iteration (ms): 460.0 | learning rate: 1.467E-06 | global batch size:     4 | lm loss: 4.425841E-01 | loss scale: 12.0 | grad norm: 38.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 3] (after 11 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 11 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 61.52 | backward-compute: 158.47 | backward-params-all-reduce: 101.08 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.88 | optimizer-unscale-and-check-inf: 19.28 | optimizer-clip-main-grad: 28.49 | optimizer-copy-main-to-model-params: 19.55 | optimizer: 130.53 | batch-generator: 0.75
==> iteration       12/      25 | consumed samples:           48 | consumed tokens:        24576 | elapsed time per iteration (ms): 457.8 | learning rate: 1.600E-06 | global batch size:     4 | lm loss: 4.055330E-01 | loss scale: 12.0 | grad norm: 37.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 2] (after 12 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0[Rank 1] (after 12 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

[Rank 0] (after 12 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 3] (after 12 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
time (ms) | forward-compute: 60.74 | backward-compute: 157.80 | backward-params-all-reduce: 100.90 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.86 | optimizer-unscale-and-check-inf: 18.94 | optimizer-clip-main-grad: 28.43 | optimizer-copy-main-to-model-params: 19.45 | optimizer: 129.87 | batch-generator: 0.72
[Rank 2] (after 13 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0[Rank 1] (after 13 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

[Rank 3] (after 13 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 13 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
==> iteration       13/      25 | consumed samples:           52 | consumed tokens:        26624 | elapsed time per iteration (ms): 457.9 | learning rate: 1.733E-06 | global batch size:     4 | lm loss: 3.153504E-01 | loss scale: 12.0 | grad norm: 5.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.47 | backward-compute: 157.58 | backward-params-all-reduce: 101.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.90 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 28.41 | optimizer-copy-main-to-model-params: 19.50 | optimizer: 130.46 | batch-generator: 0.88
[Rank 2] (after 14 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0[Rank 1] (after 14 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

==> iteration       14/      25 | consumed samples:           56 | consumed tokens:        28672 | elapsed time per iteration (ms): 458.5 | learning rate: 1.867E-06 | global batch size:     4 | lm loss: 2.597107E-01 | loss scale: 12.0 | grad norm: 5.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 3] (after 14 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 14 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 60.31 | backward-compute: 158.37 | backward-params-all-reduce: 100.96 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.87 | optimizer-unscale-and-check-inf: 19.01 | optimizer-clip-main-grad: 28.47 | optimizer-copy-main-to-model-params: 19.66 | optimizer: 130.43 | batch-generator: 0.95
[Rank 1] (after 15 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 15 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
[Rank 3] (after 15 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 15 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
==> iteration       15/      25 | consumed samples:           60 | consumed tokens:        30720 | elapsed time per iteration (ms): 459.2 | learning rate: 2.000E-06 | global batch size:     4 | lm loss: 2.258660E-01 | loss scale: 12.0 | grad norm: 4.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.70 | backward-compute: 158.41 | backward-params-all-reduce: 100.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.89 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 28.52 | optimizer-copy-main-to-model-params: 19.61 | optimizer: 130.68 | batch-generator: 0.90
[Rank 2] (after 16 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
[Rank 1] (after 16 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 3] (after 16 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 16 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
==> iteration       16/      25 | consumed samples:           64 | consumed tokens:        32768 | elapsed time per iteration (ms): 458.1 | learning rate: 2.133E-06 | global batch size:     4 | lm loss: 1.678414E-01 | loss scale: 12.0 | grad norm: 2.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.44 | backward-compute: 158.23 | backward-params-all-reduce: 100.92 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.85 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 28.38 | optimizer-copy-main-to-model-params: 19.45 | optimizer: 130.08 | batch-generator: 1.11
==> iteration       17/      25 | consumed samples:           68 | consumed tokens:        34816 | elapsed time per iteration (ms): 456.1 | learning rate: 2.267E-06 | global batch size:     4 | lm loss: 1.328247E-01 | loss scale: 12.0 | grad norm: 1.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 17 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 17 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
[Rank 3] (after 17 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 17 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 60.01 | backward-compute: 157.42 | backward-params-all-reduce: 100.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 0.71 | optimizer-unscale-and-check-inf: 18.72 | optimizer-clip-main-grad: 28.29 | optimizer-copy-main-to-model-params: 19.51 | optimizer: 129.48 | batch-generator: 0.79
[Rank 2] (after 18 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
[Rank 1] (after 18 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
==> iteration       18/      25 | consumed samples:           72 | consumed tokens:        36864 | elapsed time per iteration (ms): 458.5 | learning rate: 2.400E-06 | global batch size:     4 | lm loss: 9.733188E-02 | loss scale: 12.0 | grad norm: 1.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 18 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 3] (after 18 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
time (ms) | forward-compute: 60.60 | backward-compute: 158.41 | backward-params-all-reduce: 101.06 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.88 | optimizer-unscale-and-check-inf: 18.95 | optimizer-clip-main-grad: 28.39 | optimizer-copy-main-to-model-params: 19.46 | optimizer: 129.98 | batch-generator: 0.71
==> iteration       19/      25 | consumed samples:           76 | consumed tokens:        38912 | elapsed time per iteration (ms): 457.5 | learning rate: 2.533E-06 | global batch size:     4 | lm loss: 8.679868E-02 | loss scale: 12.0 | grad norm: 1.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 19 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 19 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
[Rank 3] (after 19 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 19 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 60.40 | backward-compute: 157.89 | backward-params-all-reduce: 101.12 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.81 | optimizer-unscale-and-check-inf: 18.78 | optimizer-clip-main-grad: 28.32 | optimizer-copy-main-to-model-params: 19.48 | optimizer: 129.68 | batch-generator: 0.64
[Rank 2] (after 20 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0[Rank 1] (after 20 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

==> iteration       20/      25 | consumed samples:           80 | consumed tokens:        40960 | elapsed time per iteration (ms): 458.6 | learning rate: 2.667E-06 | global batch size:     4 | lm loss: 8.910069E-02 | loss scale: 12.0 | grad norm: 2.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 3] (after 20 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 20 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 61.10 | backward-compute: 158.09 | backward-params-all-reduce: 100.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.88 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 28.49 | optimizer-copy-main-to-model-params: 19.48 | optimizer: 130.20 | batch-generator: 0.79
[Rank 1] (after 21 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 21 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
==> iteration       21/      25 | consumed samples:           84 | consumed tokens:        43008 | elapsed time per iteration (ms): 459.1 | learning rate: 2.800E-06 | global batch size:     4 | lm loss: 8.764230E-02 | loss scale: 12.0 | grad norm: 1.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 3] (after 21 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 21 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 60.60 | backward-compute: 158.19 | backward-params-all-reduce: 101.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.90 | optimizer-unscale-and-check-inf: 18.94 | optimizer-clip-main-grad: 28.54 | optimizer-copy-main-to-model-params: 19.58 | optimizer: 130.29 | batch-generator: 0.83
==> iteration       22/      25 | consumed samples:           88 | consumed tokens:        45056 | elapsed time per iteration (ms): 460.1 | learning rate: 2.933E-06 | global batch size:     4 | lm loss: 7.920719E-02 | loss scale: 12.0 | grad norm: 1.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 22 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 22 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
[Rank 3] (after 22 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
time (ms) | forward-compute: 60.30 | backward-compute: 159.81 | backward-params-all-reduce: 100.99 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.87 | optimizer-unscale-and-check-inf: 19.29 | optimizer-clip-main-grad: 28.37 | optimizer-copy-main-to-model-params: 19.74 | optimizer: 130.57 | batch-generator: 0.78
[Rank 0] (after 22 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 2] (after 23 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0[Rank 1] (after 23 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

==> iteration       23/      25 | consumed samples:           92 | consumed tokens:        47104 | elapsed time per iteration (ms): 443.0 | learning rate: 3.067E-06 | global batch size:     4 | lm loss: 7.670373E-02 | loss scale: 12.0 | grad norm: 0.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 23 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
[Rank 3] (after 23 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
time (ms) | forward-compute: 60.93 | backward-compute: 159.27 | backward-params-all-reduce: 101.03 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.91 | optimizer-unscale-and-check-inf: 19.26 | optimizer-clip-main-grad: 11.38 | optimizer-copy-main-to-model-params: 19.48 | optimizer: 113.34 | batch-generator: 0.83
[Rank 1] (after 24 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 2] (after 24 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
==> iteration       24/      25 | consumed samples:           96 | consumed tokens:        49152 | elapsed time per iteration (ms): 459.3 | learning rate: 3.200E-06 | global batch size:     4 | lm loss: 8.153158E-02 | loss scale: 12.0 | grad norm: 1.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 3] (after 24 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0
[Rank 0] (after 24 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 60.25 | backward-compute: 159.00 | backward-params-all-reduce: 101.24 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.87 | optimizer-unscale-and-check-inf: 18.99 | optimizer-clip-main-grad: 28.47 | optimizer-copy-main-to-model-params: 19.58 | optimizer: 130.32 | batch-generator: 0.76
[Rank 2] (after 25 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56732.0 | max reserved: 56732.0
==> iteration       25/      25 | consumed samples:          100 | consumed tokens:        51200 | elapsed time per iteration (ms): 459.6 | learning rate: 3.333E-06 | global batch size:     4 | lm loss: 7.864104E-02 | loss scale: 12.0 | grad norm: 1.162 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 25 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0[Rank 3] (after 25 iterations) memory (MB) | allocated: 55809.58203125 | max allocated: 56193.583984375 | reserved: 56746.0 | max reserved: 56746.0

[Rank 0] (after 25 iterations) memory (MB) | allocated: 55810.08203125 | max allocated: 56194.083984375 | reserved: 56660.0 | max reserved: 56660.0
time (ms) | forward-compute: 60.18 | backward-compute: 159.70 | backward-params-all-reduce: 100.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.90 | optimizer-unscale-and-check-inf: 19.23 | optimizer-clip-main-grad: 28.41 | optimizer-copy-main-to-model-params: 19.58 | optimizer: 130.47 | batch-generator: 0.76
[after training is done] datetime: 2024-03-31 16:38:24 
[2024-03-31 16:38:26,112] [INFO] [launch.py:210:main] Process 72220 exits successfully.
[2024-03-31 16:38:26,112] [INFO] [launch.py:210:main] Process 72219 exits successfully.
[2024-03-31 16:38:26,112] [INFO] [launch.py:210:main] Process 72217 exits successfully.
[2024-03-31 16:38:26,112] [INFO] [launch.py:210:main] Process 72215 exits successfully.
[2024-03-31 16:38:26,113] [INFO] [launch.py:210:main] Process 72218 exits successfully.
[2024-03-31 16:38:26,113] [INFO] [launch.py:210:main] Process 72216 exits successfully.
[2024-03-31 16:38:27,114] [INFO] [launch.py:210:main] Process 72221 exits successfully.
[2024-03-31 16:38:27,114] [INFO] [launch.py:210:main] Process 72214 exits successfully.