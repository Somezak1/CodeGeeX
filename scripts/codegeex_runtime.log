[2024-02-02 10:33:50,359] [WARNING] [runner.py:155:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-02 10:33:50,409] [INFO] [runner.py:453:main] cmd = /data0/anaconda3/envs/csw_codegeex/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29501 /data0/csw/CodeGeeX/codegeex/megatron/tools/pretrain_codegeex.py --tensor-model-parallel-size 4 --pipeline-model-parallel-size 1 --no-pipeline-parallel --num-layers 39 --hidden-size 5120 --make-vocab-size-divisible-by 52224 --num-attention-heads 40 --seq-length 512 --loss-scale 12 --max-position-embeddings 2048 --micro-batch-size 2 --global-batch-size 4 --train-iters 25 --lr 2e-4 --min-lr 1e-7 --lr-decay-iters 100000 --lr-decay-style cosine --lr-warmup-iters 1500 --log-interval 1 --eval-iters 10 --eval-interval 10 --data-path /data0/csw/CodeGeeX/pt_data/my_data --data-impl mmap --vocab-file /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json --merge-file /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt --save-interval 100 --save /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test --load-state /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/ --split 100,0,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --fp16 --ln-fp16 --attention-softmax-in-fp32 --checkpoint-activations --override-lr-scheduler --tensorboard-dir /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240202_103349 --deepspeed-activation-checkpointing --zero-stage=2 --deepspeed_config=/data0/csw/CodeGeeX/scripts/ds_config.json --no-pipeline-parallel --deepspeed
[2024-02-02 10:33:51,618] [INFO] [launch.py:96:main] 0 NCCL_IB_GID_INDEX=3
[2024-02-02 10:33:51,618] [INFO] [launch.py:96:main] 0 NCCL_IB_DISABLE=0
[2024-02-02 10:33:51,618] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-02-02 10:33:51,618] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-02-02 10:33:51,618] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-02-02 10:33:51,618] [INFO] [launch.py:123:main] dist_world_size=8
[2024-02-02 10:33:51,618] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8, data-parallel-size: 2, tensor-model-parallel size: 4, pipeline-model-parallel size: 1
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... True
  beam_search ..................................... False
  beam_search_nucleus ............................. False
  beam_warmup ..................................... False
  beam_warmup_length .............................. 0
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  co_evaluation ................................... False
  compress ........................................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 2
  data_path ....................................... ['/data0/csw/CodeGeeX/pt_data/my_data']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ /data0/csw/CodeGeeX/scripts/ds_config.json
  deepspeed_mpi ................................... False
  dist_timeout .................................... 30
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 512
  eod_mask_loss ................................... False
  eval_interval ................................... 10
  eval_iters ...................................... 10
  evaluation ...................................... False
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 20480
  finetune ........................................ False
  force_default ................................... False
  force_device .................................... None
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 4
  gold ............................................ False
  gold_beta ....................................... 0.05
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 5120
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  index_cache_dir ................................. None
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  ln_fp16 ......................................... True
  load ............................................ None
  load_state ...................................... /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 12.0
  loss_scale_window ............................... 1000
  low_memory_load ................................. None
  lr .............................................. 0.0002
  lr_decay_iters .................................. 100000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 1500
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 52224
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-07
  mmap_warmup ..................................... False
  ms_model ........................................ False
  no_learned_position_embeddings .................. False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 40
  num_beams ....................................... 4
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 39
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  play_tau ........................................ 2.0
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  reward_growth ................................... constant
  sample_rate ..................................... 1.0
  save ............................................ /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
  save_interval ................................... 100
  scale_embeddings ................................ False
  scaled_upper_triang_masked_softmax_fusion ....... False
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 512
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  shrink_embedding_gradient_alpha ................. 1.0
  shrink_embedding_gradient_steps ................. None
  shrink_logit_embedding_gradient ................. False
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tempering ....................................... None
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240202_103349
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_path .................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  train_iters ..................................... 25
  train_samples ................................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  valid_data_path ................................. None
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json
  wandb_log_interval .............................. 1
  wandb_logging ................................... False
  weight_decay .................................... 0.1
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 2
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 1967 dummy tokens (new size: 52224)
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
  > (rank=3) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
> initializing torch distributed ...
  > (rank=0) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=6) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=2) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=5) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=4) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=1) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=7) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=0) process group initialized
> initializing tensor model parallel with size 4
> initializing pipeline model parallel with size 1
  > (rank=7) process group initialized
  > (rank=3) process group initialized
  > (rank=6) process group initialized
  > (rank=2) process group initialized
  > (rank=5) process group initialized
  > (rank=4) process group initialized
  > (rank=1) process group initialized
> setting random seeds to 1234 ...
[2024-02-02 10:33:54,961] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
time to initialize megatron (seconds): 22.175
[after megatron is initialized] datetime: 2024-02-02 10:33:58
Creating output dir ...
building GPT model ...
[2024-02-02 10:33:58,248] [INFO] [utils.py:828:see_memory_usage] Before Building Model
[2024-02-02 10:33:58,249] [INFO] [utils.py:829:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB
[2024-02-02 10:33:58,249] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 60.45 GB, percent = 3.2%
Loading warmstarting model states ...
Loading model from /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/mp_rank_00_model_states.pt ...
[2024-02-02 10:34:04,016] [INFO] [utils.py:828:see_memory_usage] After Building Model
[2024-02-02 10:34:04,016] [INFO] [utils.py:829:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.23 GB         Max_CA 6 GB
[2024-02-02 10:34:04,017] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 98.55 GB, percent = 5.3%
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 3227279360
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
time (ms) | load-model-states: 4894.24
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3227279360
> learning rate decay style: cosine
DeepSpeed is enabled.
1
[2024-02-02 10:34:04,161] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.3, git-hash=unknown, git-branch=unknown
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3227279360
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 3227279360
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
[2024-02-02 10:34:04,424] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False
[2024-02-02 10:34:04,425] [INFO] [engine.py:1042:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer
[2024-02-02 10:34:04,425] [INFO] [engine.py:1048:_configure_optimizer] Using client Optimizer as basic optimizer
[2024-02-02 10:34:04,458] [INFO] [engine.py:1064:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
[2024-02-02 10:34:04,458] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2024-02-02 10:34:04,458] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2024-02-02 10:34:04,458] [INFO] [stage_1_and_2.py:132:__init__] Reduce bucket size 50000000
[2024-02-02 10:34:04,458] [INFO] [stage_1_and_2.py:133:__init__] Allgather bucket size 50000000
[2024-02-02 10:34:04,458] [INFO] [stage_1_and_2.py:134:__init__] CPU Offload: False
[2024-02-02 10:34:04,458] [INFO] [stage_1_and_2.py:135:__init__] Round robin gradient partitioning: False
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Emitting ninja build file /home/icksys/.cache/torch_extensions/py310_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.05669450759887695 seconds
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Emitting ninja build file /home/icksys/.cache/torch_extensions/py310_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.05438876152038574 seconds
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Emitting ninja build file /home/icksys/.cache/torch_extensions/py310_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.0526738166809082 seconds
Loading extension module utils...
Time to load utils op: 0.10154914855957031 seconds
Loading extension module utils...
Time to load utils op: 0.10134601593017578 seconds
Loading extension module utils...
Time to load utils op: 0.2014470100402832 seconds
Loading extension module utils...
Time to load utils op: 0.10123848915100098 seconds
Loading extension module utils...
Time to load utils op: 0.20160341262817383 seconds
Rank: 0 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
Rank: 5 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
Rank: 2 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
Rank: 1 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
Rank: 3 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
Rank: 4 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
Rank: 6 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
Rank: 7 partition count [2, 2] and sizes[(1612840960, False), (798720, False)]
[2024-02-02 10:34:14,794] [INFO] [utils.py:828:see_memory_usage] Before initializing optimizer states
[2024-02-02 10:34:14,794] [INFO] [utils.py:829:see_memory_usage] MA 12.02 GB         Max_MA 15.02 GB         CA 21.27 GB         Max_CA 21 GB
[2024-02-02 10:34:14,795] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 87.12 GB, percent = 4.7%
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...

Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003311634063720703 seconds
Time to load utils op: 0.00040984153747558594 seconds
Time to load utils op: 0.00039005279541015625 seconds
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00037670135498046875 seconds
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Time to load utils op: 0.00036263465881347656 seconds
Time to load utils op: 0.0003516674041748047 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.000392913818359375 seconds
[2024-02-02 10:34:14,901] [INFO] [utils.py:828:see_memory_usage] After initializing optimizer states
[2024-02-02 10:34:14,902] [INFO] [utils.py:829:see_memory_usage] MA 24.05 GB         Max_MA 30.06 GB         CA 39.3 GB         Max_CA 39 GB
[2024-02-02 10:34:14,902] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 87.12 GB, percent = 4.7%
[2024-02-02 10:34:14,902] [INFO] [stage_1_and_2.py:507:__init__] optimizer state initialized
[2024-02-02 10:34:14,954] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer
[2024-02-02 10:34:14,955] [INFO] [utils.py:829:see_memory_usage] MA 24.05 GB         Max_MA 24.05 GB         CA 39.3 GB         Max_CA 39 GB
[2024-02-02 10:34:14,955] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 87.11 GB, percent = 4.7%
[2024-02-02 10:34:14,955] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2024-02-02 10:34:14,955] [INFO] [engine.py:774:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2024-02-02 10:34:14,955] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <codegeex.megatron.learning_rates.AnnealingLR object at 0x7f81ef7cfa90>
[2024-02-02 10:34:14,956] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-02 10:34:14,956] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   amp_params ................... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": null,
    "exps_dir": null,
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   dump_state ................... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   global_rank .................. 0
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4096
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2024-02-02 10:34:14,957] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   optimizer_name ............... None
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   optimizer_params ............. None
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   pld_params ................... False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   scheduler_name ............... None
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   scheduler_params ............. None
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   steps_per_print .............. 5
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   tensorboard_output_path ......
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   train_batch_size ............. 4
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  2
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... True
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   world_size ................... 2
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 2,
    "contiguous_gradients": false,
    "reduce_scatter": true,
    "reduce_bucket_size": 5.000000e+07,
    "allgather_partitions": true,
    "allgather_bucket_size": 5.000000e+07,
    "overlap_comm": true,
    "load_from_fp32_weights": true,
    "elastic_checkpoint": false,
    "offload_param": null,
    "offload_optimizer": null,
    "sub_group_size": 1.000000e+09,
    "prefetch_bucket_size": 5.000000e+07,
    "param_persistence_threshold": 1.000000e+05,
    "max_live_parameters": 1.000000e+09,
    "max_reuse_distance": 1.000000e+09,
    "gather_16bit_weights_on_model_save": false,
    "ignore_unused_parameters": true,
    "round_robin_gradients": false,
    "legacy_stage1": false
}
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   zero_enabled ................. True
[2024-02-02 10:34:14,958] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 2
[2024-02-02 10:34:14,958] [INFO] [config.py:1065:print]   json = {
    "train_batch_size": 4,
    "train_micro_batch_size_per_gpu": 2,
    "steps_per_print": 5,
    "zero_optimization": {
        "stage": 2,
        "reduce_bucket_size": 5.000000e+07,
        "allgather_bucket_size": 5.000000e+07,
        "overlap_comm": true,
        "contiguous_gradients": false
    },
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 500,
        "hysteresis": 2,
        "min_loss_scale": 1,
        "initial_scale_power": 12
    },
    "wall_clock_breakdown": true
}
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0002627372741699219 seconds
FinishInitialization.
Finishparallel.
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-02-02 10:34:14
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      100
    validation: 120
    test:       40
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000204 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000099 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000090 seconds
    number of documents: 500
 > dataset split:
    train:
     document indices in [0, 500) total of 500 documents
    validation:
     document indices in [500, 500) total of 0 documents
    test:
     document indices in [500, 500) total of 0 documents
 > loading doc-idx mapping from /data0/csw/CodeGeeX/pt_data/my_data_train_indexmap_100ns_512sl_1234s_doc_idx.npy
    total number of samples: 100
    total number of epochs: 1
train_dataset:<class 'codegeex.megatron.data.prompt_dataset.PromptDataset'>
valid_dataset:<class 'NoneType'>
test_dataset:<class 'NoneType'>
> finished creating GPT datasets ...
time (ms) | model-and-optimizer-setup: 16686.31 | train/valid/test-data-iterators-setup: 738.02
[after dataloaders are built] datetime: 2024-02-02 10:34:15
done with setup ...
training ...
[before the start of training step] datetime: 2024-02-02 10:34:15
[2024-02-02 10:34:15,834] [INFO] [checkpointing.py:547:forward] Activation Checkpointing Information
[2024-02-02 10:34:15,834] [INFO] [checkpointing.py:548:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-02-02 10:34:15,834] [INFO] [checkpointing.py:551:forward] ----contiguous Memory Checkpointing False with 39 total layers
[2024-02-02 10:34:15,834] [INFO] [checkpointing.py:554:forward] ----Synchronization False
[2024-02-02 10:34:15,835] [INFO] [checkpointing.py:555:forward] ----Profiling time in checkpointing False
==> iteration        1/      25 | consumed samples:            4 | consumed tokens:         2048 | elapsed time per iteration (ms): 3110.7 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.924718E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1923.80 | backward-compute: 278.63 | optimizer: 905.18 | batch-generator: 4.90
[2024-02-02 10:34:18,936] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 4096
[2024-02-02 10:34:18,937] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1484.27 | backward_microstep: 277.85 | backward_inner_microstep: 275.86 | backward_allreduce_microstep: 1.87 | step_microstep: 907.74
[2024-02-02 10:34:18,937] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1484.29 | backward: 277.85 | backward_inner: 275.85 | backward_allreduce: 1.87 | step: 907.74
[2024-02-02 10:34:19,225] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048.0
[2024-02-02 10:34:19,226] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 61.29 | backward_microstep: 200.74 | backward_inner_microstep: 198.80 | backward_allreduce_microstep: 1.83 | step_microstep: 23.62
[2024-02-02 10:34:19,226] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 61.29 | backward: 200.74 | backward_inner: 198.80 | backward_allreduce: 1.83 | step: 23.62
==> iteration        2/      25 | consumed samples:            8 | consumed tokens:         4096 | elapsed time per iteration (ms): 291.4 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.960377E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 66.05 | backward-compute: 200.92 | optimizer: 23.76 | batch-generator: 1.18
[2024-02-02 10:34:19,511] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
==> iteration        3/      25 | consumed samples:           12 | consumed tokens:         6144 | elapsed time per iteration (ms): 286.2 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 4.465699E+00 | loss scale: 1024.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.78 | backward-compute: 198.97 | optimizer: 23.75 | batch-generator: 0.65
[2024-02-02 10:34:19,512] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 60.04 | backward_microstep: 199.35 | backward_inner_microstep: 197.41 | backward_allreduce_microstep: 1.85 | step_microstep: 24.01
[2024-02-02 10:34:19,512] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 60.05 | backward: 199.34 | backward_inner: 197.42 | backward_allreduce: 1.85 | step: 24.01
[2024-02-02 10:34:19,798] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
[2024-02-02 10:34:19,799] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 60.87 | backward_microstep: 199.55 | backward_inner_microstep: 197.63 | backward_allreduce_microstep: 1.82 | step_microstep: 23.52
[2024-02-02 10:34:19,799] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 60.86 | backward: 199.55 | backward_inner: 197.64 | backward_allreduce: 1.82 | step: 23.52
==> iteration        4/      25 | consumed samples:           16 | consumed tokens:         8192 | elapsed time per iteration (ms): 287.2 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.279449E+00 | loss scale: 512.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 63.36 | backward-compute: 199.57 | optimizer: 23.68 | batch-generator: 0.65
[2024-02-02 10:34:20,085] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2024-02-02 10:34:20,086] [INFO] [logging.py:69:log_dist] [Rank 0] step=5, skipped=5, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
==> iteration        5/      25 | consumed samples:           20 | consumed tokens:        10240 | elapsed time per iteration (ms): 287.3 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.351377E+00 | loss scale: 256.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.90 | backward-compute: 200.03 | optimizer: 23.72 | batch-generator: 0.72
[2024-02-02 10:34:20,087] [INFO] [timer.py:193:stop] 0/5, SamplesPerSec=14.031503359588921, MemAllocated=24.06GB, MaxMemAllocated=27.34GB
[2024-02-02 10:34:20,087] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.56 | backward_microstep: 200.52 | backward_inner_microstep: 198.55 | backward_allreduce_microstep: 1.89 | step_microstep: 24.18
[2024-02-02 10:34:20,087] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.57 | backward: 200.52 | backward_inner: 198.55 | backward_allreduce: 1.89 | step: 24.18
[2024-02-02 10:34:20,523] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 43.02 | optimizer_gradients: 70.84 | optimizer_step: 38.51
[2024-02-02 10:34:20,523] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.49 | backward_microstep: 198.52 | backward_inner_microstep: 196.67 | backward_allreduce_microstep: 1.77 | step_microstep: 175.48
[2024-02-02 10:34:20,523] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.49 | backward: 198.52 | backward_inner: 196.68 | backward_allreduce: 1.77 | step: 175.48
==> iteration        6/      25 | consumed samples:           24 | consumed tokens:        12288 | elapsed time per iteration (ms): 437.0 | learning rate: 1.333E-07 | global batch size:     4 | lm loss: 3.422411E+00 | loss scale: 256.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 6 iterations) memory (MB) | allocated: 24639.60400390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0
[Rank 3] (after 6 iterations) memory (MB) | allocated: 24639.60400390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0
[Rank 2] (after 6 iterations) memory (MB) | allocated: 24639.60400390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0
time (ms) | forward-compute: 62.46 | backward-compute: 198.15 | optimizer: 175.64 | batch-generator: 0.75
[Rank 0] (after 6 iterations) memory (MB) | allocated: 24638.72900390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0
[2024-02-02 10:34:20,808] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256.0, reducing to 128.0
[2024-02-02 10:34:20,809] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.49 | backward_microstep: 198.42 | backward_inner_microstep: 196.53 | backward_allreduce_microstep: 1.81 | step_microstep: 24.30
[2024-02-02 10:34:20,809] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.48 | backward: 198.42 | backward_inner: 196.54 | backward_allreduce: 1.81 | step: 24.30
==> iteration        7/      25 | consumed samples:           28 | consumed tokens:        14336 | elapsed time per iteration (ms): 285.8 | learning rate: 1.333E-07 | global batch size:     4 | lm loss: 4.032598E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.72 | backward-compute: 198.26 | optimizer: 24.59 | batch-generator: 0.73
[2024-02-02 10:34:21,231] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.88 | optimizer_gradients: 54.60 | optimizer_step: 38.44
[2024-02-02 10:34:21,232] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.94 | backward_microstep: 198.30 | backward_inner_microstep: 196.37 | backward_allreduce_microstep: 1.83 | step_microstep: 159.27
[2024-02-02 10:34:21,232] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.93 | backward: 198.30 | backward_inner: 196.39 | backward_allreduce: 1.84 | step: 159.27
==> iteration        8/      25 | consumed samples:           32 | consumed tokens:        16384 | elapsed time per iteration (ms): 422.8 | learning rate: 2.667E-07 | global batch size:     4 | lm loss: 3.425794E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.62 | backward-compute: 197.96 | optimizer: 159.37 | batch-generator: 0.74
[2024-02-02 10:34:21,651] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.85 | optimizer_gradients: 54.52 | optimizer_step: 38.40
[2024-02-02 10:34:21,651] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.24 | backward_microstep: 198.39 | backward_inner_microstep: 196.53 | backward_allreduce_microstep: 1.77 | step_microstep: 159.31
[2024-02-02 10:34:21,651] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.23 | backward: 198.39 | backward_inner: 196.54 | backward_allreduce: 1.77 | step: 159.31
==> iteration        9/      25 | consumed samples:           36 | consumed tokens:        18432 | elapsed time per iteration (ms): 419.6 | learning rate: 4.000E-07 | global batch size:     4 | lm loss: 3.614118E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.27 | backward-compute: 198.11 | optimizer: 159.40 | batch-generator: 0.59
[2024-02-02 10:34:22,105] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 43.00 | optimizer_gradients: 54.85 | optimizer_step: 38.29
[2024-02-02 10:34:22,105] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=6, lr=[5.333333333333333e-07, 5.333333333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-02 10:34:22,106] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=11.172135878512494, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
[2024-02-02 10:34:22,106] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.22 | backward_microstep: 232.01 | backward_inner_microstep: 230.02 | backward_allreduce_microstep: 1.90 | step_microstep: 160.27
[2024-02-02 10:34:22,106] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.21 | backward: 232.01 | backward_inner: 230.03 | backward_allreduce: 1.90 | step: 160.27
==> iteration       10/      25 | consumed samples:           40 | consumed tokens:        20480 | elapsed time per iteration (ms): 454.6 | learning rate: 5.333E-07 | global batch size:     4 | lm loss: 3.241038E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.64 | backward-compute: 231.66 | optimizer: 160.33 | batch-generator: 0.58
[2024-02-02 10:34:22,526] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.80 | optimizer_gradients: 54.49 | optimizer_step: 38.34
[2024-02-02 10:34:22,527] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.99 | backward_microstep: 198.77 | backward_inner_microstep: 196.90 | backward_allreduce_microstep: 1.80 | step_microstep: 159.04
[2024-02-02 10:34:22,527] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.98 | backward: 198.77 | backward_inner: 196.90 | backward_allreduce: 1.80 | step: 159.05
==> iteration       11/      25 | consumed samples:           44 | consumed tokens:        22528 | elapsed time per iteration (ms): 420.7 | learning rate: 6.667E-07 | global batch size:     4 | lm loss: 3.531645E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.31 | backward-compute: 198.42 | optimizer: 159.15 | batch-generator: 0.73
[2024-02-02 10:34:22,946] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.86 | optimizer_gradients: 54.59 | optimizer_step: 38.40
[2024-02-02 10:34:22,946] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.34 | backward_microstep: 198.39 | backward_inner_microstep: 196.50 | backward_allreduce_microstep: 1.81 | step_microstep: 159.34
[2024-02-02 10:34:22,947] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.33 | backward: 198.39 | backward_inner: 196.50 | backward_allreduce: 1.81 | step: 159.34
==> iteration       12/      25 | consumed samples:           48 | consumed tokens:        24576 | elapsed time per iteration (ms): 419.8 | learning rate: 8.000E-07 | global batch size:     4 | lm loss: 2.967396E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.50 | backward-compute: 198.04 | optimizer: 159.38 | batch-generator: 0.63
[2024-02-02 10:34:23,367] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.88 | optimizer_gradients: 54.63 | optimizer_step: 38.35
[2024-02-02 10:34:23,367] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.56 | backward_microstep: 199.05 | backward_inner_microstep: 197.16 | backward_allreduce_microstep: 1.81 | step_microstep: 159.36
[2024-02-02 10:34:23,367] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.55 | backward: 199.05 | backward_inner: 197.17 | backward_allreduce: 1.81 | step: 159.36
==> iteration       13/      25 | consumed samples:           52 | consumed tokens:        26624 | elapsed time per iteration (ms): 420.5 | learning rate: 9.333E-07 | global batch size:     4 | lm loss: 2.461848E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.47 | backward-compute: 198.72 | optimizer: 159.49 | batch-generator: 0.60
[2024-02-02 10:34:23,650] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128.0, reducing to 64.0
[2024-02-02 10:34:23,650] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.10 | backward_microstep: 197.87 | backward_inner_microstep: 196.03 | backward_allreduce_microstep: 1.76 | step_microstep: 23.66
[2024-02-02 10:34:23,651] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.09 | backward: 197.87 | backward_inner: 196.04 | backward_allreduce: 1.76 | step: 23.65
==> iteration       14/      25 | consumed samples:           56 | consumed tokens:        28672 | elapsed time per iteration (ms): 283.2 | learning rate: 9.333E-07 | global batch size:     4 | lm loss: 1.972258E+00 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.35 | backward-compute: 197.52 | optimizer: 23.76 | batch-generator: 0.61
[2024-02-02 10:34:24,070] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.89 | optimizer_gradients: 54.46 | optimizer_step: 38.35
[2024-02-02 10:34:24,070] [INFO] [logging.py:69:log_dist] [Rank 0] step=15, skipped=7, lr=[1.0666666666666667e-06, 1.0666666666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-02 10:34:24,071] [INFO] [timer.py:193:stop] 0/15, SamplesPerSec=10.78752439886692, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
[2024-02-02 10:34:24,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.44 | backward_microstep: 198.75 | backward_inner_microstep: 196.86 | backward_allreduce_microstep: 1.81 | step_microstep: 159.37
[2024-02-02 10:34:24,071] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.43 | backward: 198.75 | backward_inner: 196.86 | backward_allreduce: 1.82 | step: 159.37
==> iteration       15/      25 | consumed samples:           60 | consumed tokens:        30720 | elapsed time per iteration (ms): 420.5 | learning rate: 1.067E-06 | global batch size:     4 | lm loss: 1.510686E+00 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.12 | backward-compute: 198.35 | optimizer: 159.24 | batch-generator: 0.74
[2024-02-02 10:34:24,490] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.86 | optimizer_gradients: 54.44 | optimizer_step: 38.27
[2024-02-02 10:34:24,490] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.40 | backward_microstep: 198.67 | backward_inner_microstep: 196.74 | backward_allreduce_microstep: 1.85 | step_microstep: 159.07
[2024-02-02 10:34:24,490] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.39 | backward: 198.67 | backward_inner: 196.75 | backward_allreduce: 1.85 | step: 159.07
==> iteration       16/      25 | consumed samples:           64 | consumed tokens:        32768 | elapsed time per iteration (ms): 419.5 | learning rate: 1.200E-06 | global batch size:     4 | lm loss: 1.191236E+00 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.13 | backward-compute: 198.21 | optimizer: 159.30 | batch-generator: 0.63
[2024-02-02 10:34:24,910] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.82 | optimizer_gradients: 54.51 | optimizer_step: 38.38
[2024-02-02 10:34:24,911] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.50 | backward_microstep: 199.37 | backward_inner_microstep: 197.46 | backward_allreduce_microstep: 1.83 | step_microstep: 159.08
[2024-02-02 10:34:24,911] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.49 | backward: 199.37 | backward_inner: 197.47 | backward_allreduce: 1.82 | step: 159.08
==> iteration       17/      25 | consumed samples:           68 | consumed tokens:        34816 | elapsed time per iteration (ms): 420.6 | learning rate: 1.333E-06 | global batch size:     4 | lm loss: 6.187868E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.52 | backward-compute: 198.95 | optimizer: 159.33 | batch-generator: 0.77
[2024-02-02 10:34:25,332] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 43.08 | optimizer_gradients: 54.70 | optimizer_step: 38.37
[2024-02-02 10:34:25,332] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.08 | backward_microstep: 199.63 | backward_inner_microstep: 197.80 | backward_allreduce_microstep: 1.76 | step_microstep: 159.66
[2024-02-02 10:34:25,333] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.07 | backward: 199.63 | backward_inner: 197.81 | backward_allreduce: 1.75 | step: 159.66
==> iteration       18/      25 | consumed samples:           72 | consumed tokens:        36864 | elapsed time per iteration (ms): 421.8 | learning rate: 1.467E-06 | global batch size:     4 | lm loss: 4.847618E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.74 | backward-compute: 199.32 | optimizer: 159.87 | batch-generator: 0.60
[2024-02-02 10:34:25,753] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.91 | optimizer_gradients: 54.56 | optimizer_step: 38.32
[2024-02-02 10:34:25,753] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.90 | backward_microstep: 198.10 | backward_inner_microstep: 196.19 | backward_allreduce_microstep: 1.83 | step_microstep: 159.23
[2024-02-02 10:34:25,753] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.88 | backward: 198.10 | backward_inner: 196.19 | backward_allreduce: 1.83 | step: 159.23
==> iteration       19/      25 | consumed samples:           76 | consumed tokens:        38912 | elapsed time per iteration (ms): 420.5 | learning rate: 1.600E-06 | global batch size:     4 | lm loss: 4.176847E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.50 | backward-compute: 197.71 | optimizer: 159.45 | batch-generator: 0.70
[2024-02-02 10:34:26,172] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.92 | optimizer_gradients: 54.71 | optimizer_step: 38.41
[2024-02-02 10:34:26,172] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=7, lr=[1.7333333333333336e-06, 1.7333333333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-02 10:34:26,172] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=10.415575296835728, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
==> iteration       20/      25 | consumed samples:           80 | consumed tokens:        40960 | elapsed time per iteration (ms): 418.7 | learning rate: 1.733E-06 | global batch size:     4 | lm loss: 3.483605E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.19 | backward-compute: 197.16 | optimizer: 159.61 | batch-generator: 0.60
[2024-02-02 10:34:26,173] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.21 | backward_microstep: 197.59 | backward_inner_microstep: 195.69 | backward_allreduce_microstep: 1.81 | step_microstep: 160.05
[2024-02-02 10:34:26,173] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.20 | backward: 197.58 | backward_inner: 195.70 | backward_allreduce: 1.82 | step: 160.05
[2024-02-02 10:34:26,606] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.85 | optimizer_gradients: 55.07 | optimizer_step: 38.38
[2024-02-02 10:34:26,606] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 60.73 | backward_microstep: 207.17 | backward_inner_microstep: 205.17 | backward_allreduce_microstep: 1.91 | step_microstep: 160.05
[2024-02-02 10:34:26,607] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 60.72 | backward: 207.17 | backward_inner: 205.18 | backward_allreduce: 1.90 | step: 160.05
==> iteration       21/      25 | consumed samples:           84 | consumed tokens:        43008 | elapsed time per iteration (ms): 434.5 | learning rate: 1.867E-06 | global batch size:     4 | lm loss: 3.246801E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 67.01 | backward-compute: 206.52 | optimizer: 160.07 | batch-generator: 0.64
[2024-02-02 10:34:27,027] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.84 | optimizer_gradients: 54.75 | optimizer_step: 38.35
[2024-02-02 10:34:27,028] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.96 | backward_microstep: 199.67 | backward_inner_microstep: 197.80 | backward_allreduce_microstep: 1.78 | step_microstep: 159.59
[2024-02-02 10:34:27,028] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.95 | backward: 199.67 | backward_inner: 197.81 | backward_allreduce: 1.78 | step: 159.59
==> iteration       22/      25 | consumed samples:           88 | consumed tokens:        45056 | elapsed time per iteration (ms): 422.0 | learning rate: 2.000E-06 | global batch size:     4 | lm loss: 3.393063E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.62 | backward-compute: 199.35 | optimizer: 160.04 | batch-generator: 0.62
[2024-02-02 10:34:27,450] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.89 | optimizer_gradients: 54.76 | optimizer_step: 38.35
[2024-02-02 10:34:27,450] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.50 | backward_microstep: 199.16 | backward_inner_microstep: 197.25 | backward_allreduce_microstep: 1.83 | step_microstep: 159.51
[2024-02-02 10:34:27,450] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.49 | backward: 199.16 | backward_inner: 197.25 | backward_allreduce: 1.83 | step: 159.51
==> iteration       23/      25 | consumed samples:           92 | consumed tokens:        47104 | elapsed time per iteration (ms): 421.8 | learning rate: 2.133E-06 | global batch size:     4 | lm loss: 3.483935E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 62.38 | backward-compute: 198.72 | optimizer: 159.78 | batch-generator: 0.72
[2024-02-02 10:34:27,871] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.94 | optimizer_gradients: 54.62 | optimizer_step: 38.35
[2024-02-02 10:34:27,871] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.01 | backward_microstep: 199.15 | backward_inner_microstep: 197.30 | backward_allreduce_microstep: 1.77 | step_microstep: 159.47
[2024-02-02 10:34:27,871] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.00 | backward: 199.15 | backward_inner: 197.31 | backward_allreduce: 1.77 | step: 159.47
==> iteration       24/      25 | consumed samples:           96 | consumed tokens:        49152 | elapsed time per iteration (ms): 421.2 | learning rate: 2.267E-06 | global batch size:     4 | lm loss: 3.585826E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.81 | backward-compute: 198.87 | optimizer: 159.58 | batch-generator: 0.65
[2024-02-02 10:34:28,290] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.89 | optimizer_gradients: 54.51 | optimizer_step: 38.37
[2024-02-02 10:34:28,290] [INFO] [logging.py:69:log_dist] [Rank 0] step=25, skipped=7, lr=[2.4000000000000003e-06, 2.4000000000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-02 10:34:28,291] [INFO] [timer.py:193:stop] 0/25, SamplesPerSec=10.197704229343024, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
[2024-02-02 10:34:28,291] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.66 | backward_microstep: 197.71 | backward_inner_microstep: 195.88 | backward_allreduce_microstep: 1.75 | step_microstep: 159.60
[2024-02-02 10:34:28,291] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.65 | backward: 197.71 | backward_inner: 195.89 | backward_allreduce: 1.74 | step: 159.60
==> iteration       25/      25 | consumed samples:          100 | consumed tokens:        51200 | elapsed time per iteration (ms): 419.3 | learning rate: 2.400E-06 | global batch size:     4 | lm loss: 3.556742E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.64 | backward-compute: 197.41 | optimizer: 159.45 | batch-generator: 0.67
[after training is done] datetime: 2024-02-02 10:34:28
saving checkpoint at iteration      25 to /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-02 10:34:28,297] [INFO] [logging.py:69:log_dist] [Rank 1] Saving model checkpoint: /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/mp_rank_01_model_states.pt
[2024-02-02 10:34:28,298] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/mp_rank_00_model_states.pt
[2024-02-02 10:35:29,574] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_01_optim_states.pt
[2024-02-02 10:35:50,557] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_03_optim_states.pt
[2024-02-02 10:35:58,292] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_02_optim_states.pt
[2024-02-02 10:35:58,878] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_01_optim_states.pt
[2024-02-02 10:35:59,271] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_02_optim_states.pt
[2024-02-02 10:35:59,639] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-02 10:35:59,647] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-02-02 10:35:59,718] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_03_optim_states.pt
  successfully saved checkpoint at iteration      25 to /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
[2024-02-02 10:36:00,749] [INFO] [launch.py:210:main] Process 64732 exits successfully.
[2024-02-02 10:36:00,750] [INFO] [launch.py:210:main] Process 64729 exits successfully.
[2024-02-02 10:36:01,751] [INFO] [launch.py:210:main] Process 64727 exits successfully.
[2024-02-02 10:36:01,751] [INFO] [launch.py:210:main] Process 64731 exits successfully.
[2024-02-02 10:36:01,752] [INFO] [launch.py:210:main] Process 64728 exits successfully.
[2024-02-02 10:36:01,752] [INFO] [launch.py:210:main] Process 64730 exits successfully.
[2024-02-02 10:36:01,752] [INFO] [launch.py:210:main] Process 64733 exits successfully.
[2024-02-02 10:36:01,752] [INFO] [launch.py:210:main] Process 64734 exits successfully.
