MASTER_IP=127.0.0.1
Launching deepspeed
[2024-02-01 08:04:17,302] [WARNING] [runner.py:155:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-01 08:04:17,531] [INFO] [runner.py:453:main] cmd = /data0/anaconda3/envs/csw_codegeex/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29501 /data0/csw/CodeGeeX/codegeex/megatron/tools/pretrain_codegeex.py --tensor-model-parallel-size 4 --pipeline-model-parallel-size 1 --no-pipeline-parallel --num-layers 39 --hidden-size 5120 --make-vocab-size-divisible-by 52224 --num-attention-heads 40 --seq-length 512 --loss-scale 12 --max-position-embeddings 2048 --micro-batch-size 2 --global-batch-size 4 --train-iters 25 --lr 2e-4 --min-lr 1e-7 --lr-decay-iters 100000 --lr-decay-style cosine --lr-warmup-iters 1500 --log-interval 1 --eval-iters 10 --eval-interval 10 --data-path /data0/csw/CodeGeeX/pt_data/my_data --data-impl mmap --vocab-file /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json --merge-file /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt --save-interval 100 --save /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test --load /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test --load-state /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/ --split 100,0,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --fp16 --ln-fp16 --attention-softmax-in-fp32 --checkpoint-activations --override-lr-scheduler --tensorboard-dir /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240201_080416 --deepspeed-activation-checkpointing --zero-stage=2 --deepspeed_config=/data0/csw/CodeGeeX/scripts/ds_config.json --no-pipeline-parallel --deepspeed
[2024-02-01 08:04:18,740] [INFO] [launch.py:96:main] 0 NCCL_IB_GID_INDEX=3
[2024-02-01 08:04:18,740] [INFO] [launch.py:96:main] 0 NCCL_IB_DISABLE=0
[2024-02-01 08:04:18,740] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-02-01 08:04:18,740] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-02-01 08:04:18,740] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-02-01 08:04:18,740] [INFO] [launch.py:123:main] dist_world_size=8
[2024-02-01 08:04:18,740] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8, data-parallel-size: 2, tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... True
  beam_search ..................................... False
  beam_search_nucleus ............................. False
  beam_warmup ..................................... False
  beam_warmup_length .............................. 0
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  co_evaluation ................................... False
  compress ........................................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 2
  data_path ....................................... ['/data0/csw/CodeGeeX/pt_data/my_data']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ /data0/csw/CodeGeeX/scripts/ds_config.json
  deepspeed_mpi ................................... False
  dist_timeout .................................... 30
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 512
  eod_mask_loss ................................... False
  eval_interval ................................... 10
  eval_iters ...................................... 10
  evaluation ...................................... False
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 20480
  finetune ........................................ False
  force_default ................................... False
  force_device .................................... None
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 4
  gold ............................................ False
  gold_beta ....................................... 0.05
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 5120
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  index_cache_dir ................................. None
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  ln_fp16 ......................................... True
  load ............................................ /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
  load_state ...................................... /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 12.0
  loss_scale_window ............................... 1000
  low_memory_load ................................. None
  lr .............................................. 0.0002
  lr_decay_iters .................................. 100000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 1500
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 52224
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-07
  mmap_warmup ..................................... False
  ms_model ........................................ False
  no_learned_position_embeddings .................. False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 40
  num_beams ....................................... 4
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 39
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  play_tau ........................................ 2.0
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  reward_growth ................................... constant
  sample_rate ..................................... 1.0
  save ............................................ /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
  save_interval ................................... 100
  scale_embeddings ................................ False
  scaled_upper_triang_masked_softmax_fusion ....... False
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 512
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  shrink_embedding_gradient_alpha ................. 1.0
  shrink_embedding_gradient_steps ................. None
  shrink_logit_embedding_gradient ................. False
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tempering ....................................... None
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240201_080416
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_path .................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  train_iters ..................................... 25
  train_samples ................................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  valid_data_path ................................. None
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json
  wandb_log_interval .............................. 1
  wandb_logging ................................... False
  weight_decay .................................... 0.1
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 2
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 1967 dummy tokens (new size: 52224)
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
  > (rank=4) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=6) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=5) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=3) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
> initializing torch distributed ...
  > (rank=0) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=1) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=7) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=2) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=0) process group initialized
> initializing tensor model parallel with size 4
> initializing pipeline model parallel with size 1
  > (rank=5) process group initialized
  > (rank=3) process group initialized
  > (rank=4) process group initialized
  > (rank=1) process group initialized
  > (rank=2) process group initialized
  > (rank=7) process group initialized
  > (rank=6) process group initialized
> setting random seeds to 1234 ...
[2024-02-01 08:04:21,954] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
time to initialize megatron (seconds): 8.552
[after megatron is initialized] datetime: 2024-02-01 08:04:24 
Creating output dir ...
building GPT model ...
[2024-02-01 08:04:24,621] [INFO] [utils.py:828:see_memory_usage] Before Building Model
[2024-02-01 08:04:24,622] [INFO] [utils.py:829:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-02-01 08:04:24,625] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 56.99 GB, percent = 3.1%
Loading warmstarting model states ...
Loading model from /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/mp_rank_00_model_states.pt ...
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3227279360
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
time (ms) | load-model-states: 4811.42
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
[2024-02-01 08:04:30,547] [INFO] [utils.py:828:see_memory_usage] After Building Model
[2024-02-01 08:04:30,547] [INFO] [utils.py:829:see_memory_usage] MA 5.99 GB         Max_MA 5.99 GB         CA 6.23 GB         Max_CA 6 GB 
[2024-02-01 08:04:30,548] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 97.85 GB, percent = 5.2%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3227279360
> learning rate decay style: cosine
DeepSpeed is enabled.
1
[2024-02-01 08:04:30,670] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.3, git-hash=unknown, git-branch=unknown
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 3227279360
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 3227279360
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:552: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead
  warnings.warn(
Emitting ninja build file /home/icksys/.cache/torch_extensions/py310_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[2024-02-01 08:04:31,327] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False
[2024-02-01 08:04:31,328] [INFO] [engine.py:1042:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer
[2024-02-01 08:04:31,328] [INFO] [engine.py:1048:_configure_optimizer] Using client Optimizer as basic optimizer
[2024-02-01 08:04:31,361] [INFO] [engine.py:1064:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
[2024-02-01 08:04:31,361] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2024-02-01 08:04:31,361] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2024-02-01 08:04:31,361] [INFO] [stage_1_and_2.py:132:__init__] Reduce bucket size 50000000
[2024-02-01 08:04:31,361] [INFO] [stage_1_and_2.py:133:__init__] Allgather bucket size 50000000
[2024-02-01 08:04:31,361] [INFO] [stage_1_and_2.py:134:__init__] CPU Offload: False
[2024-02-01 08:04:31,361] [INFO] [stage_1_and_2.py:135:__init__] Round robin gradient partitioning: False
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/include -isystem /data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/include/TH -isystem /data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/include/THC -isystem /data0/anaconda3/envs/csw_codegeex/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o 
[2/2] c++ flatten_unflatten.o -shared -L/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so
Loading extension module utils...
Time to load utils op: 12.669240713119507 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 12.215662002563477 seconds
Time to load utils op: 12.215071678161621 seconds
Loading extension module utils...
Time to load utils op: 12.41622018814087 seconds
Loading extension module utils...
Time to load utils op: 12.716354370117188 seconds
Loading extension module utils...
Time to load utils op: 12.415815830230713 seconds
Loading extension module utils...
Time to load utils op: 12.41586971282959 seconds
Loading extension module utils...
Time to load utils op: 12.515150547027588 seconds
Rank: 7 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
Rank: 1 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
Rank: 3 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
Rank: 5 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
Rank: 2 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
Rank: 0 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
Rank: 4 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
Rank: 6 partition count [2, 2] and sizes[(1612840960, False), (798720, False)] 
[2024-02-01 08:04:57,085] [INFO] [utils.py:828:see_memory_usage] Before initializing optimizer states
[2024-02-01 08:04:57,086] [INFO] [utils.py:829:see_memory_usage] MA 12.02 GB         Max_MA 15.02 GB         CA 21.27 GB         Max_CA 21 GB 
[2024-02-01 08:04:57,086] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 82.31 GB, percent = 4.4%
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Time to load utils op: 0.00038123130798339844 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00037169456481933594 seconds
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...Time to load utils op: 0.00033926963806152344 seconds

Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.00031065940856933594 secondsLoading extension module utils...

Time to load utils op: 0.0003676414489746094 seconds
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Time to load utils op: 0.00038695335388183594 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003819465637207031 seconds
[2024-02-01 08:04:57,317] [INFO] [utils.py:828:see_memory_usage] After initializing optimizer states
[2024-02-01 08:04:57,317] [INFO] [utils.py:829:see_memory_usage] MA 24.05 GB         Max_MA 30.06 GB         CA 39.3 GB         Max_CA 39 GB 
[2024-02-01 08:04:57,318] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 82.3 GB, percent = 4.4%
[2024-02-01 08:04:57,318] [INFO] [stage_1_and_2.py:507:__init__] optimizer state initialized
[2024-02-01 08:04:57,363] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer
[2024-02-01 08:04:57,363] [INFO] [utils.py:829:see_memory_usage] MA 24.05 GB         Max_MA 24.05 GB         CA 39.3 GB         Max_CA 39 GB 
[2024-02-01 08:04:57,363] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 82.3 GB, percent = 4.4%
[2024-02-01 08:04:57,364] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2024-02-01 08:04:57,364] [INFO] [engine.py:774:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2024-02-01 08:04:57,364] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <codegeex.megatron.learning_rates.AnnealingLR object at 0x7f2463587dc0>
[2024-02-01 08:04:57,364] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-01 08:04:57,364] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   amp_params ................... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   dump_state ................... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   global_rank .................. 0
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4096
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   optimizer_name ............... None
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   optimizer_params ............. None
[2024-02-01 08:04:57,365] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   pld_params ................... False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   scheduler_name ............... None
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   scheduler_params ............. None
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   steps_per_print .............. 5
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   tensorboard_output_path ...... 
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   train_batch_size ............. 4
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  2
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... True
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   world_size ................... 2
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 2, 
    "contiguous_gradients": false, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+07, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+07, 
    "overlap_comm": true, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": false, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_16bit_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   zero_enabled ................. True
[2024-02-01 08:04:57,366] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 2
[2024-02-01 08:04:57,366] [INFO] [config.py:1065:print]   json = {
    "train_batch_size": 4, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 5, 
    "zero_optimization": {
        "stage": 2, 
        "reduce_bucket_size": 5.000000e+07, 
        "allgather_bucket_size": 5.000000e+07, 
        "overlap_comm": true, 
        "contiguous_gradients": false
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 12
    }, 
    "wall_clock_breakdown": true
}
Using /home/icksys/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00025200843811035156 seconds
FinishInitialization.
Finishparallel.
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
WARNING: could not find the metadata file /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test 
    will not load any checkpoints and will start from random
Rank 0 loaded checkpoint and waiting for other ranks
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Rank 2 loaded checkpoint and waiting for other ranks
Rank 5 loaded checkpoint and waiting for other ranks
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2024-02-01 08:04:57,367] [WARNING] [engine.py:2449:load_checkpoint] Unable to find latest file at /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Rank 3 loaded checkpoint and waiting for other ranks
Rank 4 loaded checkpoint and waiting for other ranks
Rank 6 loaded checkpoint and waiting for other ranksRank 1 loaded checkpoint and waiting for other ranksRank 7 loaded checkpoint and waiting for other ranks


time (ms) | load-checkpoint: 0.50
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-02-01 08:04:57 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      100
    validation: 120
    test:       40
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000181 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000084 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000076 seconds
    number of documents: 500
 > dataset split:
    train:
     document indices in [0, 500) total of 500 documents
    validation:
     document indices in [500, 500) total of 0 documents
    test:
     document indices in [500, 500) total of 0 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > elasped time to build and save doc-idx mapping (seconds): 0.000311
 > loading doc-idx mapping from /data0/csw/CodeGeeX/pt_data/my_data_train_indexmap_100ns_512sl_1234s_doc_idx.npy
    total number of samples: 100
    total number of epochs: 1
train_dataset:<class 'codegeex.megatron.data.prompt_dataset.PromptDataset'>
valid_dataset:<class 'NoneType'>
test_dataset:<class 'NoneType'>
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-02-01 08:04:58 time (ms) | model-and-optimizer-setup: 32815.52 | train/valid/test-data-iterators-setup: 1236.59

done with setup ...
training ...
[before the start of training step] datetime: 2024-02-01 08:04:58 
[2024-02-01 08:04:58,733] [INFO] [checkpointing.py:547:forward] Activation Checkpointing Information
[2024-02-01 08:04:58,733] [INFO] [checkpointing.py:548:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-02-01 08:04:58,733] [INFO] [checkpointing.py:551:forward] ----contiguous Memory Checkpointing False with 39 total layers
[2024-02-01 08:04:58,733] [INFO] [checkpointing.py:554:forward] ----Synchronization False
[2024-02-01 08:04:58,733] [INFO] [checkpointing.py:555:forward] ----Profiling time in checkpointing False
[2024-02-01 08:05:01,117] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 4096
[2024-02-01 08:05:01,118] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1447.16 | backward_microstep: 215.27 | backward_inner_microstep: 213.18 | backward_allreduce_microstep: 1.97 | step_microstep: 637.58
[2024-02-01 08:05:01,118] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 1447.17 | backward: 215.26 | backward_inner: 213.19 | backward_allreduce: 1.97 | step: 637.58
==> iteration        1/      25 | consumed samples:            4 | consumed tokens:         2048 | elapsed time per iteration (ms): 2413.6 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.924718E+00 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1544.35 | backward-compute: 216.06 | optimizer: 650.21 | batch-generator: 7.03
[2024-02-01 08:05:01,417] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048.0
[2024-02-01 08:05:01,418] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 62.31 | backward_microstep: 198.89 | backward_inner_microstep: 196.63 | backward_allreduce_microstep: 2.17 | step_microstep: 23.92
[2024-02-01 08:05:01,418] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 62.32 | backward: 198.89 | backward_inner: 196.63 | backward_allreduce: 2.17 | step: 23.92
==> iteration        2/      25 | consumed samples:            8 | consumed tokens:         4096 | elapsed time per iteration (ms): 287.3 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.960377E+00 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 63.23 | backward-compute: 199.38 | optimizer: 23.97 | batch-generator: 0.98
[2024-02-01 08:05:01,702] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
[2024-02-01 08:05:01,703] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 60.62 | backward_microstep: 197.71 | backward_inner_microstep: 195.75 | backward_allreduce_microstep: 1.87 | step_microstep: 23.65
[2024-02-01 08:05:01,703] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 60.63 | backward: 197.71 | backward_inner: 195.76 | backward_allreduce: 1.87 | step: 23.65
==> iteration        3/      25 | consumed samples:           12 | consumed tokens:         6144 | elapsed time per iteration (ms): 284.6 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 4.465699E+00 | loss scale: 1024.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.86 | backward-compute: 198.08 | optimizer: 23.99 | batch-generator: 0.76
[2024-02-01 08:05:01,983] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
[2024-02-01 08:05:01,984] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.41 | backward_microstep: 194.76 | backward_inner_microstep: 192.79 | backward_allreduce_microstep: 1.88 | step_microstep: 23.60
[2024-02-01 08:05:01,984] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.42 | backward: 194.75 | backward_inner: 192.80 | backward_allreduce: 1.89 | step: 23.60
==> iteration        4/      25 | consumed samples:           16 | consumed tokens:         8192 | elapsed time per iteration (ms): 281.1 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.279449E+00 | loss scale: 512.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.43 | backward-compute: 195.18 | optimizer: 23.83 | batch-generator: 0.71
[2024-02-01 08:05:02,262] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512.0, reducing to 256.0
[2024-02-01 08:05:02,263] [INFO] [logging.py:69:log_dist] [Rank 0] step=5, skipped=5, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-01 08:05:02,264] [INFO] [timer.py:193:stop] 0/5, SamplesPerSec=14.280154809627343, MemAllocated=24.06GB, MaxMemAllocated=27.34GB
==> iteration        5/      25 | consumed samples:           20 | consumed tokens:        10240 | elapsed time per iteration (ms): 279.9 | learning rate: 0.000E+00 | global batch size:     4 | lm loss: 3.351377E+00 | loss scale: 256.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.90 | backward-compute: 194.06 | optimizer: 24.17 | batch-generator: 0.74
[2024-02-01 08:05:02,264] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.57 | backward_microstep: 193.48 | backward_inner_microstep: 191.02 | backward_allreduce_microstep: 2.37 | step_microstep: 24.67
[2024-02-01 08:05:02,264] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.58 | backward: 193.48 | backward_inner: 191.03 | backward_allreduce: 2.37 | step: 24.67
[2024-02-01 08:05:02,695] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 43.12 | optimizer_gradients: 71.31 | optimizer_step: 38.51
[2024-02-01 08:05:02,696] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.21 | backward_microstep: 194.01 | backward_inner_microstep: 191.68 | backward_allreduce_microstep: 2.24 | step_microstep: 176.22
[2024-02-01 08:05:02,696] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.20 | backward: 194.01 | backward_inner: 191.69 | backward_allreduce: 2.24 | step: 176.22
==> iteration        6/      25 | consumed samples:           24 | consumed tokens:        12288 | elapsed time per iteration (ms): 433.3 | learning rate: 1.333E-07 | global batch size:     4 | lm loss: 3.422411E+00 | loss scale: 256.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[Rank 1] (after 6 iterations) memory (MB) | allocated: 24639.60400390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0

[Rank 0] (after 6 iterations) memory (MB) | allocated: 24638.72900390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0
[Rank 2] (after 6 iterations) memory (MB) | allocated: 24639.60400390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0
[Rank 3] (after 6 iterations) memory (MB) | allocated: 24639.60400390625 | max allocated: 36972.3779296875 | reserved: 46544.0 | max reserved: 46544.0
time (ms) | forward-compute: 61.15 | backward-compute: 194.35 | optimizer: 176.77 | batch-generator: 0.61
[2024-02-01 08:05:02,975] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256.0, reducing to 128.0
[2024-02-01 08:05:02,976] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.96 | backward_microstep: 193.39 | backward_inner_microstep: 191.11 | backward_allreduce_microstep: 2.19 | step_microstep: 24.02
[2024-02-01 08:05:02,976] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.96 | backward: 193.39 | backward_inner: 191.12 | backward_allreduce: 2.20 | step: 24.02
==> iteration        7/      25 | consumed samples:           28 | consumed tokens:        14336 | elapsed time per iteration (ms): 279.7 | learning rate: 1.333E-07 | global batch size:     4 | lm loss: 4.032598E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.19 | backward-compute: 193.89 | optimizer: 24.28 | batch-generator: 0.68
[2024-02-01 08:05:03,391] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.92 | optimizer_gradients: 54.83 | optimizer_step: 38.45
[2024-02-01 08:05:03,392] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 60.11 | backward_microstep: 193.67 | backward_inner_microstep: 191.32 | backward_allreduce_microstep: 2.27 | step_microstep: 159.65
[2024-02-01 08:05:03,392] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 60.10 | backward: 193.67 | backward_inner: 191.33 | backward_allreduce: 2.27 | step: 159.65
==> iteration        8/      25 | consumed samples:           32 | consumed tokens:        16384 | elapsed time per iteration (ms): 415.8 | learning rate: 2.667E-07 | global batch size:     4 | lm loss: 3.425794E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 61.20 | backward-compute: 193.51 | optimizer: 160.12 | batch-generator: 0.66
[2024-02-01 08:05:03,804] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.96 | optimizer_gradients: 54.73 | optimizer_step: 38.31
[2024-02-01 08:05:03,805] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.38 | backward_microstep: 192.20 | backward_inner_microstep: 189.97 | backward_allreduce_microstep: 2.14 | step_microstep: 159.59
[2024-02-01 08:05:03,805] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.37 | backward: 192.20 | backward_inner: 189.98 | backward_allreduce: 2.15 | step: 159.59
==> iteration        9/      25 | consumed samples:           36 | consumed tokens:        18432 | elapsed time per iteration (ms): 413.0 | learning rate: 4.000E-07 | global batch size:     4 | lm loss: 3.614118E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.11 | backward-compute: 192.08 | optimizer: 159.98 | batch-generator: 0.64
[2024-02-01 08:05:04,218] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.87 | optimizer_gradients: 54.62 | optimizer_step: 38.35
[2024-02-01 08:05:04,218] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=6, lr=[5.333333333333333e-07, 5.333333333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-01 08:05:04,219] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=11.48797448397391, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
[2024-02-01 08:05:04,219] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.75 | backward_microstep: 193.89 | backward_inner_microstep: 191.95 | backward_allreduce_microstep: 1.85 | step_microstep: 159.69
[2024-02-01 08:05:04,219] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.74 | backward: 193.89 | backward_inner: 191.96 | backward_allreduce: 1.86 | step: 159.69
==> iteration       10/      25 | consumed samples:           40 | consumed tokens:        20480 | elapsed time per iteration (ms): 413.6 | learning rate: 5.333E-07 | global batch size:     4 | lm loss: 3.241038E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 58.95 | backward-compute: 193.98 | optimizer: 159.87 | batch-generator: 0.60
[2024-02-01 08:05:04,630] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.94 | optimizer_gradients: 54.68 | optimizer_step: 38.46
[2024-02-01 08:05:04,631] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.98 | backward_microstep: 192.01 | backward_inner_microstep: 189.68 | backward_allreduce_microstep: 2.25 | step_microstep: 159.53
[2024-02-01 08:05:04,631] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.99 | backward: 192.01 | backward_inner: 189.69 | backward_allreduce: 2.25 | step: 159.53
==> iteration       11/      25 | consumed samples:           44 | consumed tokens:        22528 | elapsed time per iteration (ms): 412.1 | learning rate: 6.667E-07 | global batch size:     4 | lm loss: 3.531645E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.25 | backward-compute: 192.18 | optimizer: 159.85 | batch-generator: 0.59
[2024-02-01 08:05:05,042] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.94 | optimizer_gradients: 54.62 | optimizer_step: 38.45
[2024-02-01 08:05:05,042] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.97 | backward_microstep: 192.04 | backward_inner_microstep: 189.79 | backward_allreduce_microstep: 2.17 | step_microstep: 159.39
[2024-02-01 08:05:05,043] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.96 | backward: 192.04 | backward_inner: 189.79 | backward_allreduce: 2.17 | step: 159.40
==> iteration       12/      25 | consumed samples:           48 | consumed tokens:        24576 | elapsed time per iteration (ms): 411.8 | learning rate: 8.000E-07 | global batch size:     4 | lm loss: 2.967396E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.22 | backward-compute: 191.99 | optimizer: 159.75 | batch-generator: 0.56
[2024-02-01 08:05:05,453] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.81 | optimizer_gradients: 54.64 | optimizer_step: 38.34
[2024-02-01 08:05:05,454] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.99 | backward_microstep: 191.74 | backward_inner_microstep: 189.60 | backward_allreduce_microstep: 2.05 | step_microstep: 159.38
[2024-02-01 08:05:05,454] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.98 | backward: 191.74 | backward_inner: 189.61 | backward_allreduce: 2.06 | step: 159.38
==> iteration       13/      25 | consumed samples:           52 | consumed tokens:        26624 | elapsed time per iteration (ms): 411.4 | learning rate: 9.333E-07 | global batch size:     4 | lm loss: 2.461848E+00 | loss scale: 128.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.05 | backward-compute: 191.66 | optimizer: 159.95 | batch-generator: 0.56
[2024-02-01 08:05:05,728] [INFO] [stage_1_and_2.py:1651:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128.0, reducing to 64.0
[2024-02-01 08:05:05,729] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.19 | backward_microstep: 191.05 | backward_inner_microstep: 188.88 | backward_allreduce_microstep: 2.08 | step_microstep: 23.69
[2024-02-01 08:05:05,729] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.18 | backward: 191.04 | backward_inner: 188.88 | backward_allreduce: 2.09 | step: 23.69
==> iteration       14/      25 | consumed samples:           56 | consumed tokens:        28672 | elapsed time per iteration (ms): 274.4 | learning rate: 9.333E-07 | global batch size:     4 | lm loss: 1.972258E+00 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.16 | backward-compute: 190.67 | optimizer: 23.98 | batch-generator: 0.56
[2024-02-01 08:05:06,140] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 43.09 | optimizer_gradients: 54.50 | optimizer_step: 38.36
[2024-02-01 08:05:06,140] [INFO] [logging.py:69:log_dist] [Rank 0] step=15, skipped=7, lr=[1.0666666666666667e-06, 1.0666666666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-01 08:05:06,141] [INFO] [timer.py:193:stop] 0/15, SamplesPerSec=11.067336283793415, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
[2024-02-01 08:05:06,141] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.38 | backward_microstep: 192.17 | backward_inner_microstep: 189.96 | backward_allreduce_microstep: 2.13 | step_microstep: 159.59
[2024-02-01 08:05:06,141] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.37 | backward: 192.17 | backward_inner: 189.97 | backward_allreduce: 2.13 | step: 159.59
==> iteration       15/      25 | consumed samples:           60 | consumed tokens:        30720 | elapsed time per iteration (ms): 412.0 | learning rate: 1.067E-06 | global batch size:     4 | lm loss: 1.510686E+00 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.75 | backward-compute: 191.87 | optimizer: 159.67 | batch-generator: 0.70
[2024-02-01 08:05:06,549] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.82 | optimizer_gradients: 54.68 | optimizer_step: 38.33
[2024-02-01 08:05:06,550] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.57 | backward_microstep: 190.39 | backward_inner_microstep: 188.22 | backward_allreduce_microstep: 2.09 | step_microstep: 159.12
[2024-02-01 08:05:06,550] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.56 | backward: 190.39 | backward_inner: 188.22 | backward_allreduce: 2.09 | step: 159.12
==> iteration       16/      25 | consumed samples:           64 | consumed tokens:        32768 | elapsed time per iteration (ms): 409.7 | learning rate: 1.200E-06 | global batch size:     4 | lm loss: 1.191236E+00 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.08 | backward-compute: 190.25 | optimizer: 159.64 | batch-generator: 0.52
[2024-02-01 08:05:06,961] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.94 | optimizer_gradients: 54.59 | optimizer_step: 38.42
[2024-02-01 08:05:06,962] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.36 | backward_microstep: 192.55 | backward_inner_microstep: 190.29 | backward_allreduce_microstep: 2.18 | step_microstep: 159.55
[2024-02-01 08:05:06,962] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.35 | backward: 192.55 | backward_inner: 190.29 | backward_allreduce: 2.18 | step: 159.55
==> iteration       17/      25 | consumed samples:           68 | consumed tokens:        34816 | elapsed time per iteration (ms): 411.1 | learning rate: 1.333E-06 | global batch size:     4 | lm loss: 6.187868E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 58.37 | backward-compute: 192.26 | optimizer: 159.80 | batch-generator: 0.56
[2024-02-01 08:05:07,372] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.87 | optimizer_gradients: 54.61 | optimizer_step: 38.41
[2024-02-01 08:05:07,372] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 58.11 | backward_microstep: 190.93 | backward_inner_microstep: 188.78 | backward_allreduce_microstep: 2.07 | step_microstep: 159.31
[2024-02-01 08:05:07,372] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 58.10 | backward: 190.92 | backward_inner: 188.78 | backward_allreduce: 2.07 | step: 159.31
==> iteration       18/      25 | consumed samples:           72 | consumed tokens:        36864 | elapsed time per iteration (ms): 411.4 | learning rate: 1.467E-06 | global batch size:     4 | lm loss: 4.847618E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.73 | backward-compute: 190.75 | optimizer: 159.95 | batch-generator: 0.71
[2024-02-01 08:05:07,786] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.90 | optimizer_gradients: 56.18 | optimizer_step: 38.32
[2024-02-01 08:05:07,786] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.67 | backward_microstep: 191.60 | backward_inner_microstep: 189.29 | backward_allreduce_microstep: 2.22 | step_microstep: 161.25
[2024-02-01 08:05:07,786] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.66 | backward: 191.60 | backward_inner: 189.30 | backward_allreduce: 2.22 | step: 161.25
==> iteration       19/      25 | consumed samples:           76 | consumed tokens:        38912 | elapsed time per iteration (ms): 414.0 | learning rate: 1.600E-06 | global batch size:     4 | lm loss: 4.176847E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.36 | backward-compute: 191.83 | optimizer: 161.90 | batch-generator: 0.69
[2024-02-01 08:05:08,198] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.92 | optimizer_gradients: 54.73 | optimizer_step: 38.33
[2024-02-01 08:05:08,198] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=7, lr=[1.7333333333333336e-06, 1.7333333333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-01 08:05:08,199] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=10.6706244937688, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
[2024-02-01 08:05:08,199] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 57.87 | backward_microstep: 192.11 | backward_inner_microstep: 189.84 | backward_allreduce_microstep: 2.19 | step_microstep: 159.95
[2024-02-01 08:05:08,199] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 57.87 | backward: 192.11 | backward_inner: 189.84 | backward_allreduce: 2.19 | step: 159.95
==> iteration       20/      25 | consumed samples:           80 | consumed tokens:        40960 | elapsed time per iteration (ms): 412.1 | learning rate: 1.733E-06 | global batch size:     4 | lm loss: 3.483605E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 59.12 | backward-compute: 192.10 | optimizer: 159.98 | batch-generator: 0.64
[2024-02-01 08:05:08,610] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.79 | optimizer_gradients: 54.70 | optimizer_step: 38.33
[2024-02-01 08:05:08,611] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.08 | backward_microstep: 191.68 | backward_inner_microstep: 189.51 | backward_allreduce_microstep: 2.07 | step_microstep: 159.24
[2024-02-01 08:05:08,611] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.07 | backward: 191.68 | backward_inner: 189.53 | backward_allreduce: 2.08 | step: 159.24
==> iteration       21/      25 | consumed samples:           84 | consumed tokens:        43008 | elapsed time per iteration (ms): 412.2 | learning rate: 1.867E-06 | global batch size:     4 | lm loss: 3.246801E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.30 | backward-compute: 191.40 | optimizer: 159.75 | batch-generator: 0.62
[2024-02-01 08:05:09,023] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.96 | optimizer_gradients: 54.85 | optimizer_step: 38.43
[2024-02-01 08:05:09,024] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.17 | backward_microstep: 192.01 | backward_inner_microstep: 189.73 | backward_allreduce_microstep: 2.20 | step_microstep: 159.72
[2024-02-01 08:05:09,024] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.16 | backward: 192.01 | backward_inner: 189.74 | backward_allreduce: 2.20 | step: 159.71
==> iteration       22/      25 | consumed samples:           88 | consumed tokens:        45056 | elapsed time per iteration (ms): 412.8 | learning rate: 2.000E-06 | global batch size:     4 | lm loss: 3.393063E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.12 | backward-compute: 191.79 | optimizer: 160.05 | batch-generator: 0.57
[2024-02-01 08:05:09,436] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.95 | optimizer_gradients: 54.74 | optimizer_step: 38.31
[2024-02-01 08:05:09,436] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.11 | backward_microstep: 192.04 | backward_inner_microstep: 189.82 | backward_allreduce_microstep: 2.15 | step_microstep: 159.64
[2024-02-01 08:05:09,437] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.10 | backward: 192.04 | backward_inner: 189.82 | backward_allreduce: 2.15 | step: 159.64
==> iteration       23/      25 | consumed samples:           92 | consumed tokens:        47104 | elapsed time per iteration (ms): 413.1 | learning rate: 2.133E-06 | global batch size:     4 | lm loss: 3.483935E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.02 | backward-compute: 191.70 | optimizer: 160.49 | batch-generator: 0.60
[2024-02-01 08:05:09,855] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 42.89 | optimizer_gradients: 55.87 | optimizer_step: 38.35
[2024-02-01 08:05:09,855] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.54 | backward_microstep: 193.19 | backward_inner_microstep: 190.99 | backward_allreduce_microstep: 2.10 | step_microstep: 164.07
[2024-02-01 08:05:09,855] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.53 | backward: 193.18 | backward_inner: 191.00 | backward_allreduce: 2.11 | step: 164.07
==> iteration       24/      25 | consumed samples:           96 | consumed tokens:        49152 | elapsed time per iteration (ms): 418.4 | learning rate: 2.267E-06 | global batch size:     4 | lm loss: 3.585826E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.10 | backward-compute: 193.21 | optimizer: 164.18 | batch-generator: 0.69
[2024-02-01 08:05:10,267] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | optimizer_allgather: 43.02 | optimizer_gradients: 54.78 | optimizer_step: 38.33
[2024-02-01 08:05:10,267] [INFO] [logging.py:69:log_dist] [Rank 0] step=25, skipped=7, lr=[2.4000000000000003e-06, 2.4000000000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-02-01 08:05:10,268] [INFO] [timer.py:193:stop] 0/25, SamplesPerSec=10.445221753768566, MemAllocated=24.06GB, MaxMemAllocated=36.11GB
[2024-02-01 08:05:10,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 59.23 | backward_microstep: 191.56 | backward_inner_microstep: 189.31 | backward_allreduce_microstep: 2.17 | step_microstep: 159.99
[2024-02-01 08:05:10,268] [INFO] [logging.py:69:log_dist] [Rank 0] rank=0 time (ms) | forward: 59.22 | backward: 191.56 | backward_inner: 189.31 | backward_allreduce: 2.17 | step: 160.00
==> iteration       25/      25 | consumed samples:          100 | consumed tokens:        51200 | elapsed time per iteration (ms): 412.4 | learning rate: 2.400E-06 | global batch size:     4 | lm loss: 3.556742E-01 | loss scale: 64.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 60.10 | backward-compute: 191.39 | optimizer: 160.00 | batch-generator: 0.58
[after training is done] datetime: 2024-02-01 08:05:10 
saving checkpoint at iteration      25 to /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data0/anaconda3/envs/csw_codegeex/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-02-01 08:05:10,275] [INFO] [logging.py:69:log_dist] [Rank 1] Saving model checkpoint: /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/mp_rank_01_model_states.pt
[2024-02-01 08:05:10,275] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/mp_rank_00_model_states.pt
[2024-02-01 08:05:57,075] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_02_optim_states.pt
[2024-02-01 08:05:57,928] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_01_optim_states.pt
[2024-02-01 08:06:02,123] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_03_optim_states.pt
[2024-02-01 08:06:02,537] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-02-01 08:06:02,558] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_02_optim_states.pt
[2024-02-01 08:06:03,258] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_01_optim_states.pt
[2024-02-01 08:06:03,434] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_1_mp_rank_03_optim_states.pt
[2024-02-01 08:06:03,615] [INFO] [engine.py:3058:_save_zero_checkpoint] zero checkpoint saved /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/global_step25/zero_pp_rank_0_mp_rank_00_optim_states.pt
  successfully saved checkpoint at iteration      25 to /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
[2024-02-01 08:06:04,864] [INFO] [launch.py:210:main] Process 9095 exits successfully.
[2024-02-01 08:06:04,864] [INFO] [launch.py:210:main] Process 9092 exits successfully.
[2024-02-01 08:06:04,864] [INFO] [launch.py:210:main] Process 9094 exits successfully.
[2024-02-01 08:06:05,865] [INFO] [launch.py:210:main] Process 9096 exits successfully.
[2024-02-01 08:06:05,865] [INFO] [launch.py:210:main] Process 9093 exits successfully.
[2024-02-01 08:06:05,865] [INFO] [launch.py:210:main] Process 9098 exits successfully.
[2024-02-01 08:06:05,865] [INFO] [launch.py:210:main] Process 9099 exits successfully.
[2024-02-01 08:06:05,865] [INFO] [launch.py:210:main] Process 9097 exits successfully.