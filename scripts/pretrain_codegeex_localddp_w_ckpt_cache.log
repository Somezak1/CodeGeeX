[2024-03-31 16:41:42,674] [WARNING] [runner.py:155:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-03-31 16:41:42,725] [INFO] [runner.py:453:main] cmd = /data0/anaconda3/envs/csw_codegeex/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29501 /data0/csw/CodeGeeX/codegeex/megatron/tools/pretrain_codegeex.py --tensor-model-parallel-size 4 --pipeline-model-parallel-size 1 --no-pipeline-parallel --num-layers 39 --hidden-size 5120 --make-vocab-size-divisible-by 52224 --num-attention-heads 40 --seq-length 512 --loss-scale 12 --max-position-embeddings 2048 --micro-batch-size 2 --global-batch-size 4 --train-iters 25 --lr 2e-4 --min-lr 1e-7 --lr-decay-iters 100000 --lr-decay-style cosine --lr-warmup-iters 1500 --log-interval 1 --eval-iters 10 --eval-interval 10 --data-path /data0/csw/CodeGeeX/pt_data/my_data --data-impl mmap --vocab-file /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json --merge-file /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt --save-interval 100 --save /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test --load-state /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/ --split 100,0,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --fp16 --ln-fp16 --attention-softmax-in-fp32 --checkpoint-activations --override-lr-scheduler --tensorboard-dir /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240331_164141 --use-contiguous-buffers-in-ddp --accumulate-allreduce-grads-in-fp32 --distribute-checkpointed-activations
[2024-03-31 16:41:43,935] [INFO] [launch.py:96:main] 0 NCCL_IB_GID_INDEX=3
[2024-03-31 16:41:43,935] [INFO] [launch.py:96:main] 0 NCCL_IB_DISABLE=0
[2024-03-31 16:41:43,935] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-03-31 16:41:43,935] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-03-31 16:41:43,935] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-03-31 16:41:43,935] [INFO] [launch.py:123:main] dist_world_size=8
[2024-03-31 16:41:43,935] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
using world size: 8, data-parallel-size: 2, tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... True
  beam_search ..................................... False
  beam_search_nucleus ............................. False
  beam_warmup ..................................... False
  beam_warmup_length .............................. 0
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  co_evaluation ................................... False
  compress ........................................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 2
  data_path ....................................... ['/data0/csw/CodeGeeX/pt_data/my_data']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... False
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ None
  deepspeed_mpi ................................... False
  dist_timeout .................................... 30
  distribute_checkpointed_activations ............. True
  distributed_backend ............................. nccl
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 512
  eod_mask_loss ................................... False
  eval_interval ................................... 10
  eval_iters ...................................... 10
  evaluation ...................................... False
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 20480
  finetune ........................................ False
  force_default ................................... False
  force_device .................................... None
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 4
  gold ............................................ False
  gold_beta ....................................... 0.05
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 5120
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  index_cache_dir ................................. None
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  ln_fp16 ......................................... True
  load ............................................ None
  load_state ...................................... /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 12.0
  loss_scale_window ............................... 1000
  low_memory_load ................................. None
  lr .............................................. 0.0002
  lr_decay_iters .................................. 100000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 1500
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 52224
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-07
  mmap_warmup ..................................... False
  ms_model ........................................ False
  no_learned_position_embeddings .................. False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 40
  num_beams ....................................... 4
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 39
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  play_tau ........................................ 2.0
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  reward_growth ................................... constant
  sample_rate ..................................... 1.0
  save ............................................ /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test
  save_interval ................................... 100
  scale_embeddings ................................ False
  scaled_upper_triang_masked_softmax_fusion ....... False
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 512
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  shrink_embedding_gradient_alpha ................. 1.0
  shrink_embedding_gradient_steps ................. None
  shrink_logit_embedding_gradient ................. False
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tempering ....................................... None
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /data0/csw/CodeGeeX/scripts/pretrain-codegeex-13b-test/tb20240331_164141
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_path .................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  train_iters ..................................... 25
  train_samples ................................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... True
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  valid_data_path ................................. None
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /data0/csw/CodeGeeX/codegeex/tokenizer/vocab.json
  wandb_log_interval .............................. 1
  wandb_logging ................................... False
  weight_decay .................................... 0.1
  world_size ...................................... 8
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 1967 dummy tokens (new size: 52224)
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
  > (rank=2) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=1) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
> initializing torch distributed ...
  > (rank=0) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=3) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=6) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=7) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=4) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=5) initializing process group: world_size=8 backend=nccl init_method=tcp://127.0.0.1:29501
  > (rank=0) process group initialized
> initializing tensor model parallel with size 4
> initializing pipeline model parallel with size 1
  > (rank=1) process group initialized
  > (rank=2) process group initialized
  > (rank=3) process group initialized
  > (rank=6) process group initialized
  > (rank=4) process group initialized
  > (rank=5) process group initialized
  > (rank=7) process group initialized
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> building the checkpointed activations memory buffer with 204472320 num elements and torch.float16 dtype (390.0 MB)...
time to initialize megatron (seconds): -48.721
[after megatron is initialized] datetime: 2024-03-31 16:41:51 
Creating output dir ...
building GPT model ...
[2024-03-31 16:41:51,362] [INFO] [utils.py:828:see_memory_usage] Before Building Model
[2024-03-31 16:41:51,364] [INFO] [utils.py:829:see_memory_usage] MA 0.38 GB         Max_MA 0.38 GB         CA 0.38 GB         Max_CA 0 GB 
[2024-03-31 16:41:51,364] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 61.96 GB, percent = 3.3%
Loading warmstarting model states ...
Loading model from /data0/csw/CodeGeeX/scripts/mp4_parallel_weights/mp_rank_00_model_states.pt ...
[2024-03-31 16:41:57,313] [INFO] [utils.py:828:see_memory_usage] After Building Model
[2024-03-31 16:41:57,313] [INFO] [utils.py:829:see_memory_usage] MA 6.37 GB         Max_MA 6.37 GB         CA 6.61 GB         Max_CA 7 GB 
[2024-03-31 16:41:57,314] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 111.12 GB, percent = 5.9%
time (ms) | load-model-states: 5031.96
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3227279360
[2024-03-31 16:41:57,667] [INFO] [utils.py:828:see_memory_usage] Before moving to GPU
[2024-03-31 16:41:57,668] [INFO] [utils.py:829:see_memory_usage] MA 6.37 GB         Max_MA 6.37 GB         CA 6.61 GB         Max_CA 7 GB 
[2024-03-31 16:41:57,668] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 92.09 GB, percent = 4.9%
 > moving model to GPU ...
 > moving to GPU done
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 3227279360
 > moving model to GPU ...
 > moving model to GPU ...
 > converting to fp16 done
 > creating DDP model ...
 > moving to GPU done
 > converting model to fp16 ...
 > moving to GPU done
 > converting model to fp16 ...
 > moving model to GPU ...
 > converting to fp16 done
 > creating DDP model ...
 > converting to fp16 done
 > creating DDP model ...
 > creating DDP model done
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
 > creating DDP model done
 > creating DDP model done
[2024-03-31 16:41:57,778] [INFO] [utils.py:828:see_memory_usage] After moving to GPU
[2024-03-31 16:41:57,778] [INFO] [utils.py:829:see_memory_usage] MA 6.39 GB         Max_MA 6.39 GB         CA 6.63 GB         Max_CA 7 GB 
[2024-03-31 16:41:57,778] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 87.87 GB, percent = 4.7%
 > creating DDP model done
[2024-03-31 16:41:57,866] [INFO] [utils.py:828:see_memory_usage] Before Float16Module Warpped
[2024-03-31 16:41:57,866] [INFO] [utils.py:829:see_memory_usage] MA 6.39 GB         Max_MA 6.39 GB         CA 6.63 GB         Max_CA 7 GB 
[2024-03-31 16:41:57,867] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 85.64 GB, percent = 4.6%
 > converting model to fp16 ...
 > converting to fp16 done
[2024-03-31 16:41:57,958] [INFO] [utils.py:828:see_memory_usage] After Float16Module Warpped
[2024-03-31 16:41:57,959] [INFO] [utils.py:829:see_memory_usage] MA 6.39 GB         Max_MA 6.39 GB         CA 6.63 GB         Max_CA 7 GB 
[2024-03-31 16:41:57,959] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 82.95 GB, percent = 4.4%
 > creating DDP model ...
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
[2024-03-31 16:41:58,021] [INFO] [utils.py:828:see_memory_usage] Before LocalDDP Wrapped
[2024-03-31 16:41:58,022] [INFO] [utils.py:829:see_memory_usage] MA 6.39 GB         Max_MA 6.39 GB         CA 6.63 GB         Max_CA 7 GB 
[2024-03-31 16:41:58,022] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 81.13 GB, percent = 4.3%
 > creating DDP model done
 > creating DDP model done
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 3227279360
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
[2024-03-31 16:41:58,105] [INFO] [utils.py:828:see_memory_usage] After LocalDDP Wrapped
[2024-03-31 16:41:58,106] [INFO] [utils.py:829:see_memory_usage] MA 18.42 GB         Max_MA 18.42 GB         CA 18.65 GB         Max_CA 19 GB 
[2024-03-31 16:41:58,106] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 79.52 GB, percent = 4.3%
 > creating DDP model done
[2024-03-31 16:41:58,155] [INFO] [utils.py:828:see_memory_usage] Before Adam Init
[2024-03-31 16:41:58,156] [INFO] [utils.py:829:see_memory_usage] MA 18.42 GB         Max_MA 18.42 GB         CA 18.65 GB         Max_CA 19 GB 
[2024-03-31 16:41:58,156] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 78.84 GB, percent = 4.2%
[2024-03-31 16:41:58,205] [INFO] [utils.py:828:see_memory_usage] After Adam Init
[2024-03-31 16:41:58,206] [INFO] [utils.py:829:see_memory_usage] MA 18.42 GB         Max_MA 18.42 GB         CA 18.65 GB         Max_CA 19 GB 
[2024-03-31 16:41:58,206] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 78.17 GB, percent = 4.2%
[2024-03-31 16:41:58,253] [INFO] [utils.py:828:see_memory_usage] Before Float16OptimizerWithFloat16Params Wrapped
[2024-03-31 16:41:58,254] [INFO] [utils.py:829:see_memory_usage] MA 18.42 GB         Max_MA 18.42 GB         CA 18.65 GB         Max_CA 19 GB 
[2024-03-31 16:41:58,254] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.52 GB, percent = 4.1%
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3227279360
 > moving model to GPU ...
 > moving to GPU done
 > converting model to fp16 ...
 > converting to fp16 done
 > creating DDP model ...
 > creating DDP model done
[2024-03-31 16:41:58,426] [INFO] [utils.py:828:see_memory_usage] After Float16OptimizerWithFloat16Params Wrapped
[2024-03-31 16:41:58,427] [INFO] [utils.py:829:see_memory_usage] MA 30.52 GB         Max_MA 30.56 GB         CA 30.83 GB         Max_CA 31 GB 
[2024-03-31 16:41:58,427] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 77.41 GB, percent = 4.1%
> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-31 16:41:58 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      100
    validation: 120
    test:       40
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000220 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000085 seconds
    number of documents: 500
 > building dataset index ...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.015664 seconds
    number of documents: 500
 > dataset split:
    train:
     document indices in [0, 500) total of 500 documents
    validation:
     document indices in [500, 500) total of 0 documents
    test:
     document indices in [500, 500) total of 0 documents
 > loading doc-idx mapping from /data0/csw/CodeGeeX/pt_data/my_data_train_indexmap_100ns_512sl_1234s_doc_idx.npy
    total number of samples: 100
    total number of epochs: 1
train_dataset:<class 'codegeex.megatron.data.prompt_dataset.PromptDataset'>
valid_dataset:<class 'NoneType'>
test_dataset:<class 'NoneType'>
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-03-31 16:42:00 time (ms) | model-and-optimizer-setup: 6608.54 | train/valid/test-data-iterators-setup: 1567.33

done with setup ...
training ...
[before the start of training step] datetime: 2024-03-31 16:42:00 
==> iteration        1/      25 | consumed samples:            4 | consumed tokens:         2048 | elapsed time per iteration (ms): 3594.3 | learning rate: 1.333E-07 | global batch size:     4 | lm loss: 3.924718E+00 | loss scale: 12.0 | grad norm: 732.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1867.83 | backward-compute: 169.18 | backward-params-all-reduce: 102.12 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.79 | optimizer-unscale-and-check-inf: 654.65 | optimizer-clip-main-grad: 28.79 | optimizer-copy-main-to-model-params: 19.60 | optimizer: 1446.22 | batch-generator: 5.09
[Rank 1] (after 1 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56199.58251953125 | reserved: 56582.0 | max reserved: 56582.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56197.58251953125 | reserved: 56510.0 | max reserved: 56510.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56199.58251953125 | reserved: 56582.0 | max reserved: 56582.0
[Rank 2] (after 1 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56199.58251953125 | reserved: 56582.0 | max reserved: 56582.0
==> iteration        2/      25 | consumed samples:            8 | consumed tokens:         4096 | elapsed time per iteration (ms): 512.3 | learning rate: 2.667E-07 | global batch size:     4 | lm loss: 3.960377E+00 | loss scale: 12.0 | grad norm: 642.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 104.46 | backward-compute: 167.92 | backward-params-all-reduce: 101.83 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.81 | optimizer-unscale-and-check-inf: 18.94 | optimizer-clip-main-grad: 28.43 | optimizer-copy-main-to-model-params: 19.44 | optimizer: 129.78 | batch-generator: 0.93
[Rank 1] (after 2 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 2 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 3] (after 2 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0[Rank 2] (after 2 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0

==> iteration        3/      25 | consumed samples:           12 | consumed tokens:         6144 | elapsed time per iteration (ms): 467.4 | learning rate: 4.000E-07 | global batch size:     4 | lm loss: 4.450614E+00 | loss scale: 12.0 | grad norm: 593.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.73 | backward-compute: 163.38 | backward-params-all-reduce: 101.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.93 | optimizer-unscale-and-check-inf: 18.64 | optimizer-clip-main-grad: 28.45 | optimizer-copy-main-to-model-params: 19.48 | optimizer: 129.82 | batch-generator: 0.61
[Rank 2] (after 3 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 3 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 3 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 3 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration        4/      25 | consumed samples:           16 | consumed tokens:         8192 | elapsed time per iteration (ms): 467.2 | learning rate: 5.333E-07 | global batch size:     4 | lm loss: 3.256634E+00 | loss scale: 12.0 | grad norm: 601.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 65.75 | backward-compute: 161.31 | backward-params-all-reduce: 102.02 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.79 | optimizer-unscale-and-check-inf: 18.99 | optimizer-clip-main-grad: 28.36 | optimizer-copy-main-to-model-params: 19.48 | optimizer: 129.84 | batch-generator: 0.65
[Rank 2] (after 4 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 4 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 4 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 4 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration        5/      25 | consumed samples:           20 | consumed tokens:        10240 | elapsed time per iteration (ms): 466.4 | learning rate: 6.667E-07 | global batch size:     4 | lm loss: 3.165217E+00 | loss scale: 12.0 | grad norm: 448.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 65.70 | backward-compute: 160.78 | backward-params-all-reduce: 102.58 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 0.64 | optimizer-unscale-and-check-inf: 18.52 | optimizer-clip-main-grad: 28.26 | optimizer-copy-main-to-model-params: 19.45 | optimizer: 129.10 | batch-generator: 0.62
[Rank 2] (after 5 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 5 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 5 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 5 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration        6/      25 | consumed samples:           24 | consumed tokens:        12288 | elapsed time per iteration (ms): 467.3 | learning rate: 8.000E-07 | global batch size:     4 | lm loss: 2.749013E+00 | loss scale: 12.0 | grad norm: 349.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 65.96 | backward-compute: 161.61 | backward-params-all-reduce: 101.03 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.79 | optimizer-unscale-and-check-inf: 19.43 | optimizer-clip-main-grad: 28.37 | optimizer-copy-main-to-model-params: 19.47 | optimizer: 130.43 | batch-generator: 0.64
[Rank 1] (after 6 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 2] (after 6 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 6 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 6 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
==> iteration        7/      25 | consumed samples:           28 | consumed tokens:        14336 | elapsed time per iteration (ms): 464.2 | learning rate: 9.333E-07 | global batch size:     4 | lm loss: 2.593580E+00 | loss scale: 12.0 | grad norm: 290.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.14 | backward-compute: 161.25 | backward-params-all-reduce: 100.85 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.65 | optimizer-unscale-and-check-inf: 19.22 | optimizer-clip-main-grad: 28.27 | optimizer-copy-main-to-model-params: 19.41 | optimizer: 129.80 | batch-generator: 0.65
[Rank 2] (after 7 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 7 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0[Rank 3] (after 7 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0

[Rank 0] (after 7 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
==> iteration        8/      25 | consumed samples:           32 | consumed tokens:        16384 | elapsed time per iteration (ms): 466.0 | learning rate: 1.067E-06 | global batch size:     4 | lm loss: 1.495719E+00 | loss scale: 12.0 | grad norm: 157.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 65.03 | backward-compute: 161.28 | backward-params-all-reduce: 101.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.68 | optimizer-unscale-and-check-inf: 19.22 | optimizer-clip-main-grad: 28.35 | optimizer-copy-main-to-model-params: 19.48 | optimizer: 129.95 | batch-generator: 0.61
[Rank 2] (after 8 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 8 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 8 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 8 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration        9/      25 | consumed samples:           36 | consumed tokens:        18432 | elapsed time per iteration (ms): 466.3 | learning rate: 1.200E-06 | global batch size:     4 | lm loss: 1.228085E+00 | loss scale: 12.0 | grad norm: 177.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.76 | backward-compute: 162.09 | backward-params-all-reduce: 101.06 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.75 | optimizer-unscale-and-check-inf: 19.27 | optimizer-clip-main-grad: 28.32 | optimizer-copy-main-to-model-params: 19.47 | optimizer: 130.12 | batch-generator: 0.65
[Rank 2] (after 9 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 9 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0[Rank 1] (after 9 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0[Rank 0] (after 9 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0


==> iteration       10/      25 | consumed samples:           40 | consumed tokens:        20480 | elapsed time per iteration (ms): 465.6 | learning rate: 1.333E-06 | global batch size:     4 | lm loss: 5.464401E-01 | loss scale: 12.0 | grad norm: 66.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.56 | backward-compute: 159.70 | backward-params-all-reduce: 103.43 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.75 | optimizer-unscale-and-check-inf: 19.07 | optimizer-clip-main-grad: 28.29 | optimizer-copy-main-to-model-params: 19.41 | optimizer: 129.78 | batch-generator: 0.63
[Rank 0] (after 10 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 2] (after 10 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 10 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 10 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       11/      25 | consumed samples:           44 | consumed tokens:        22528 | elapsed time per iteration (ms): 466.3 | learning rate: 1.467E-06 | global batch size:     4 | lm loss: 4.425841E-01 | loss scale: 12.0 | grad norm: 38.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.72 | backward-compute: 160.24 | backward-params-all-reduce: 103.25 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.67 | optimizer-unscale-and-check-inf: 19.25 | optimizer-clip-main-grad: 28.26 | optimizer-copy-main-to-model-params: 19.46 | optimizer: 129.84 | batch-generator: 0.64
[Rank 0] (after 11 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 11 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0[Rank 2] (after 11 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0

[Rank 3] (after 11 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       12/      25 | consumed samples:           48 | consumed tokens:        24576 | elapsed time per iteration (ms): 464.5 | learning rate: 1.600E-06 | global batch size:     4 | lm loss: 4.055330E-01 | loss scale: 12.0 | grad norm: 37.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.55 | backward-compute: 159.78 | backward-params-all-reduce: 102.41 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.61 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 28.27 | optimizer-copy-main-to-model-params: 19.37 | optimizer: 129.53 | batch-generator: 0.72
[Rank 2] (after 12 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 12 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 12 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 12 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       13/      25 | consumed samples:           52 | consumed tokens:        26624 | elapsed time per iteration (ms): 465.7 | learning rate: 1.733E-06 | global batch size:     4 | lm loss: 3.153504E-01 | loss scale: 12.0 | grad norm: 5.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.47 | backward-compute: 159.27 | backward-params-all-reduce: 104.21 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 0.63 | optimizer-unscale-and-check-inf: 19.15 | optimizer-clip-main-grad: 28.26 | optimizer-copy-main-to-model-params: 19.44 | optimizer: 129.60 | batch-generator: 0.58
[Rank 2] (after 13 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 13 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 13 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 13 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
==> iteration       14/      25 | consumed samples:           56 | consumed tokens:        28672 | elapsed time per iteration (ms): 467.6 | learning rate: 1.867E-06 | global batch size:     4 | lm loss: 2.597107E-01 | loss scale: 12.0 | grad norm: 5.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 66.06 | backward-compute: 160.15 | backward-params-all-reduce: 103.41 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.75 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 28.23 | optimizer-copy-main-to-model-params: 19.37 | optimizer: 129.63 | batch-generator: 0.79
[Rank 2] (after 14 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 14 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 14 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 14 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       15/      25 | consumed samples:           60 | consumed tokens:        30720 | elapsed time per iteration (ms): 466.7 | learning rate: 2.000E-06 | global batch size:     4 | lm loss: 2.258660E-01 | loss scale: 12.0 | grad norm: 4.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 65.32 | backward-compute: 160.99 | backward-params-all-reduce: 102.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.66 | optimizer-unscale-and-check-inf: 18.77 | optimizer-clip-main-grad: 28.33 | optimizer-copy-main-to-model-params: 19.50 | optimizer: 129.50 | batch-generator: 0.59
[Rank 2] (after 15 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 15 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 15 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 15 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       16/      25 | consumed samples:           64 | consumed tokens:        32768 | elapsed time per iteration (ms): 466.4 | learning rate: 2.133E-06 | global batch size:     4 | lm loss: 1.678414E-01 | loss scale: 12.0 | grad norm: 2.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.80 | backward-compute: 160.04 | backward-params-all-reduce: 103.83 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.65 | optimizer-unscale-and-check-inf: 19.06 | optimizer-clip-main-grad: 28.31 | optimizer-copy-main-to-model-params: 19.36 | optimizer: 129.59 | batch-generator: 0.64
[Rank 2] (after 16 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 16 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 16 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 16 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       17/      25 | consumed samples:           68 | consumed tokens:        34816 | elapsed time per iteration (ms): 468.2 | learning rate: 2.267E-06 | global batch size:     4 | lm loss: 1.328247E-01 | loss scale: 12.0 | grad norm: 1.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 65.08 | backward-compute: 160.41 | backward-params-all-reduce: 103.99 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.78 | optimizer-unscale-and-check-inf: 19.39 | optimizer-clip-main-grad: 28.33 | optimizer-copy-main-to-model-params: 19.60 | optimizer: 130.34 | batch-generator: 0.61
[Rank 0] (after 17 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 3] (after 17 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0[Rank 1] (after 17 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0

[Rank 2] (after 17 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       18/      25 | consumed samples:           72 | consumed tokens:        36864 | elapsed time per iteration (ms): 467.1 | learning rate: 2.400E-06 | global batch size:     4 | lm loss: 9.733188E-02 | loss scale: 12.0 | grad norm: 1.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.52 | backward-compute: 161.34 | backward-params-all-reduce: 103.01 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.83 | optimizer-unscale-and-check-inf: 19.04 | optimizer-clip-main-grad: 28.34 | optimizer-copy-main-to-model-params: 19.45 | optimizer: 129.84 | batch-generator: 0.67
[Rank 0] (after 18 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 2] (after 18 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 18 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 18 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       19/      25 | consumed samples:           76 | consumed tokens:        38912 | elapsed time per iteration (ms): 468.1 | learning rate: 2.533E-06 | global batch size:     4 | lm loss: 8.679868E-02 | loss scale: 12.0 | grad norm: 1.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.85 | backward-compute: 164.40 | backward-params-all-reduce: 101.03 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.89 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 28.26 | optimizer-copy-main-to-model-params: 19.46 | optimizer: 129.52 | batch-generator: 0.59
[Rank 2] (after 19 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 19 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 19 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 19 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       20/      25 | consumed samples:           80 | consumed tokens:        40960 | elapsed time per iteration (ms): 465.9 | learning rate: 2.667E-06 | global batch size:     4 | lm loss: 8.910069E-02 | loss scale: 12.0 | grad norm: 2.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.79 | backward-compute: 162.43 | backward-params-all-reduce: 100.99 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.78 | optimizer-unscale-and-check-inf: 18.64 | optimizer-clip-main-grad: 28.29 | optimizer-copy-main-to-model-params: 19.42 | optimizer: 129.36 | batch-generator: 0.68
[Rank 0] (after 20 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 2] (after 20 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 20 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 20 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       21/      25 | consumed samples:           84 | consumed tokens:        43008 | elapsed time per iteration (ms): 462.9 | learning rate: 2.800E-06 | global batch size:     4 | lm loss: 8.764230E-02 | loss scale: 12.0 | grad norm: 1.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.47 | backward-compute: 157.64 | backward-params-all-reduce: 103.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 0.58 | optimizer-unscale-and-check-inf: 18.99 | optimizer-clip-main-grad: 28.20 | optimizer-copy-main-to-model-params: 19.38 | optimizer: 129.35 | batch-generator: 0.59
[Rank 2] (after 21 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 21 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0[Rank 0] (after 21 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0

[Rank 3] (after 21 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       22/      25 | consumed samples:           88 | consumed tokens:        45056 | elapsed time per iteration (ms): 465.4 | learning rate: 2.933E-06 | global batch size:     4 | lm loss: 7.920719E-02 | loss scale: 12.0 | grad norm: 1.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 65.34 | backward-compute: 158.43 | backward-params-all-reduce: 102.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 0.82 | optimizer-unscale-and-check-inf: 19.63 | optimizer-clip-main-grad: 28.37 | optimizer-copy-main-to-model-params: 19.51 | optimizer: 130.60 | batch-generator: 0.60
[Rank 0] (after 22 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 3] (after 22 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 22 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 2] (after 22 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       23/      25 | consumed samples:           92 | consumed tokens:        47104 | elapsed time per iteration (ms): 444.8 | learning rate: 3.067E-06 | global batch size:     4 | lm loss: 7.670373E-02 | loss scale: 12.0 | grad norm: 0.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.04 | backward-compute: 158.77 | backward-params-all-reduce: 101.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 0.66 | optimizer-unscale-and-check-inf: 18.80 | optimizer-clip-main-grad: 11.21 | optimizer-copy-main-to-model-params: 19.35 | optimizer: 112.12 | batch-generator: 0.66
[Rank 3] (after 23 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 1] (after 23 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 23 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 2] (after 23 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       24/      25 | consumed samples:           96 | consumed tokens:        49152 | elapsed time per iteration (ms): 462.4 | learning rate: 3.200E-06 | global batch size:     4 | lm loss: 8.153158E-02 | loss scale: 12.0 | grad norm: 1.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 63.75 | backward-compute: 158.46 | backward-params-all-reduce: 102.33 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 0.61 | optimizer-unscale-and-check-inf: 19.07 | optimizer-clip-main-grad: 28.31 | optimizer-copy-main-to-model-params: 19.54 | optimizer: 129.66 | batch-generator: 0.59
[Rank 0] (after 24 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0
[Rank 1] (after 24 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 3] (after 24 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 2] (after 24 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
==> iteration       25/      25 | consumed samples:          100 | consumed tokens:        51200 | elapsed time per iteration (ms): 463.2 | learning rate: 3.333E-06 | global batch size:     4 | lm loss: 7.864104E-02 | loss scale: 12.0 | grad norm: 1.162 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 64.38 | backward-compute: 158.70 | backward-params-all-reduce: 101.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 0.85 | optimizer-unscale-and-check-inf: 19.23 | optimizer-clip-main-grad: 28.24 | optimizer-copy-main-to-model-params: 19.44 | optimizer: 130.04 | batch-generator: 0.75
[Rank 1] (after 25 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 2] (after 25 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0
[Rank 0] (after 25 iterations) memory (MB) | allocated: 56197.58203125 | max allocated: 56581.583984375 | reserved: 56964.0 | max reserved: 56964.0[Rank 3] (after 25 iterations) memory (MB) | allocated: 56199.58203125 | max allocated: 56583.583984375 | reserved: 57022.0 | max reserved: 57022.0

[after training is done] datetime: 2024-03-31 16:42:14 
[2024-03-31 16:42:15,970] [INFO] [launch.py:210:main] Process 91290 exits successfully.
[2024-03-31 16:42:16,972] [INFO] [launch.py:210:main] Process 91285 exits successfully.
[2024-03-31 16:42:16,972] [INFO] [launch.py:210:main] Process 91292 exits successfully.
[2024-03-31 16:42:16,972] [INFO] [launch.py:210:main] Process 91288 exits successfully.
[2024-03-31 16:42:16,972] [INFO] [launch.py:210:main] Process 91291 exits successfully.
[2024-03-31 16:42:16,973] [INFO] [launch.py:210:main] Process 91289 exits successfully.
[2024-03-31 16:42:16,973] [INFO] [launch.py:210:main] Process 91287 exits successfully.
[2024-03-31 16:42:17,974] [INFO] [launch.py:210:main] Process 91286 exits successfully.